---
layout: post
title: Deep Learning- It works in practice, but does it work in theory?
date: '2020-01-01T17:52:42-04:00'
tags: []
---

![My helpful screenshot](/assets/uc_theory_practice.jpg)

For a long time I was not impressed with deep learning. In graduate school I was pretty entrenched in the statistical machine learning school of thought. Plus, Chicago is a place where people walk around wearing shirts that say "it works in practice, but does it work in theory?". 

My ML bible is still [Elements of Statistical Learning](https://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576/ref=sr_1_2?keywords=statistical+learning&qid=1579795046&sr=8-2).  

From the classical SML perspective, deep learning is not that impressive; in fact, it is problematic both from the statistical and computational perspective.

## Generalization Error

Suppose we have $n$ i.i.d. realizations of the pair $$(x_i,y_i)$$, where $$x_i$$ is a $$D$$-dimensional real-valued vector $$y_i$$ is generated by

\\[y_i = f(x_i) + \epsilon_i \\]
\\[ x \leq O(n^{2\beta/(2\beta + D)}), \\]

$$\epsilon_i$$ is Gaussian noise and $$f$$ is a $$D$$ a function $$f: \mathbb{R}^D\rightarrow \mathbb{R}$$. To have any hope to learn $$f$$ we need to make some assumptions on its structure. In the classical nonparametric literature, you put some constraints on the smoothness of $$f$$, such that it belongs to a Holder class ()

This is actually a bad result. Say we want to know how many samples it would take to learn a function with two continuous derivatives, bounding the estimation error to $$\kappa$$. The number of samples required is

\\[ n = \kappa^{-1-D/4}. \\]


The number of required samples grows **exponentially** with the number of features! In applications where the number of inputs is in the thousands, this means our generalization error in the minimax sense will be pretty bad.


Statistically

Up to today, there is a big gap between theoretical and applied research progress for deep learning. This is a deviation from past trends in ML, such as [kernel methods](https://dl.acm.org/doi/book/10.5555/559923), which has deep roots in theory, and has enjoyed interest from both practitioners and theoreticians.

Deep learning models for natural language applications was a critical component of our product at [x.ai](http://x.ai)

I went to a presentation at JSM this summer by one of the co-authors of [this](https://arxiv.org/pdf/1802.04474.pdf) paper. For 

> ReLU functions can approximate step functions, and a composition of the step functions in
a combination of other parts of the network can easily express smooth functions restricted
to pieces. In contrast, even though the other methods have the universal approximation
property, they require a larger number of parameters to approximate non-smooth structures.

markdown this is a tests



### The nonconvexity problem
![nonconvex](/assets/nonconvex.png)


The second complaint about DL still stands. One still needs to solve a gnarly, non-convex optimization problem to get the above guarantees. Generally speaking, when you try to solve a non-convex problem, you are at great risk of getting stuck in the domain of attraction of a local optimum. The local optimum could be much worse than the global optimum. There are a lot of techniques for "jumping out" 

There have been a few papers in recent years tackling this question, such as [Essentially No Barriers in Neural Network Energy Landscape](https://icml.cc/Conferences/2018/Schedule?showEvent=2780) by Draxler et. al. The gist is that local minima tend to be pretty good for neural nets -- either local minima are also global minima, or they are essentially just as good as the global minimum.
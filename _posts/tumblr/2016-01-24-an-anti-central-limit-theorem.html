---
layout: post
title: An anti-central limit theorem
date: '2016-01-24T21:54:16-05:00'
tags: [central-limit-theorem]
tumblr_url: https://ericjanofsky.com/post/137992772862/an-anti-central-limit-theorem
---
<p>The basic version of the central limit theorem says that for an i.i.d. sample from a distribution with finite variance \(\sigma^2\) and mean \(\mu\), the re-scaled sample mean converges to a standard normal distribution:</p><p>\[ \sqrt{N} (\bar{X} - \mu) \rightarrow Normal(0,\sigma^2). \]</p><p>Basically the CLT says that the error in the sample mean in estimating the location parameter decreases at a rate of \(O(N^{-&frac12;})\) with the sample size. The CLT is what makes statistical inference possible. Some parts of the above statement may be relaxed, but if you get rid of the assumption of finite variance, things can get weird.</p><p>The Cauchy distribution with location \(\mu\) and scale \(\sigma\) is a distribution on \((-\infty,\infty)\) with density</p><p>\[ f(x) = \frac{1}{\sigma\pi(1+(\frac{x-\mu}{\sigma})^2}. \]</p><p>It is well-known that the sample mean \(\bar{X}\) of an i.i.d. sample \(X_1,\ldots,X_N\) from a Cauchy distribution has the same law as a single sample, \(\bar{X}\overset{D}{=} X_1\). And so the sample mean does not improve as an estimate of the location as the number of samples increases.</p><p>A surprising fact is there are distributions where the sample mean gets <i>worse </i>as an estimate of location as the number of samples increases.</p><p>Let \(X\sim Normal(0,1)\), and let \(Y=\frac{1}{X^2}\). Applying a change of variables we find that \(Y\) has distribution function</p><p>\[ F(y) = 2\left(1-\Phi\left(\frac{1}{\sqrt{y}}\right)\right), \]</p><p>where \(\Phi\) is the standard normal c.d.f.</p><p>Define \(F_a(y) = F(y/a)\) to be the scale family of densities for \(F\). Consider i.i.d. \(Y_1,Y_2\sim F=F_1\). It is possible to verify that the convolution \(Y_1+Y_2 \sim F_2\) (I will leave that as an exercise).</p><p>What does this tell us? Consider the probability</p><p>\[ P\left(\frac{Y_1+Y_2}{4} \leq y\right)  = 2 \left(1 - \Phi\left(\frac{2}{(4y)^{&frac12;}}\right) \right)= F_1(y).\]</p><p>Thus, the law of the sample mean with two samples has the same law as <i>twice</i> any of the individual samples.</p><p>In general, we get that the sample mean has the same law as a sample <i>scaled by the number of samples</i>:<br/></p><p>\[ \bar{Y} \overset{D}{=} N Y_1. \]</p><p>Instead of the error decreasing, the the error of the sample mean in estimating the location <i>increases at a rate of \(O(N)\) </i>with the number of samples!</p><p>Postscript: This distribution is called the <a href="https://en.wikipedia.org/wiki/L%C3%A9vy_distribution">Lévy</a> distribution and the unusual behavior of the sample mean (for Cauchy as well) is described by the <a href="https://en.wikipedia.org/wiki/Stable_distribution#A_generalized_central_limit_theorem">generalized central limit theorem</a>. The Lévy, Cauchy and Gaussian distribution are all <i>stable </i>distributions, which means the families are each closed under averaging. For the Gaussian family, the law of the sample mean follows \(\bar{X}\overset{D}{=} N^{-&frac12;} X_1\).</p>

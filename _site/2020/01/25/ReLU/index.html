<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Deep Learning- It works in practice, but does it work in theory?</title>
  <meta name="description" content="There is a big gap between theoretical and applied research progress for deep learning. This is a deviation from past trends in ML, such as kernel methods, which has deep roots in theory, and has enjoyed interest from both practitioners and theoreticians.">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/2020/01/25/ReLU/">
  
  
  <link rel="alternate" type="application/rss+xml" title="Eric Janofsky" href="http://localhost:4000/feed.xml">

  

  
  <meta property="og:title" content="Deep Learning- It works in practice, but does it work in theory?">
  <meta property="og:site_name" content="Eric Janofsky">
  <meta property="og:url" content="http://localhost:4000/2020/01/25/ReLU/">
  <meta property="og:description" content="There is a big gap between theoretical and applied research progress for deep learning. This is a deviation from past trends in ML, such as kernel methods, which has deep roots in theory, and has enjoyed interest from both practitioners and theoreticians.">
  
  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="Deep Learning- It works in practice, but does it work in theory?">
  <meta name="twitter:description" content="There is a big gap between theoretical and applied research progress for deep learning. This is a deviation from past trends in ML, such as kernel methods, which has deep roots in theory, and has e...">
  
  

  <link rel="dns-prefetch" href="https://fonts.gstatic.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css?family=Bitter:400,400i,700&amp;display=swap" rel="stylesheet">

  
  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-156801543-1', 'auto');
    ga('send', 'pageview');

  </script>


</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Eric Janofsky</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/archives/">Archives</a>
      
        
        <a class="page-link" href="https://github.com/geb5101h">GitHub</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>There is a big gap between theoretical and applied research progress for deep learning. This is a deviation from past trends in ML, such as <a href="https://dl.acm.org/doi/book/10.5555/559923">kernel methods</a>, which has deep roots in theory, and has enjoyed interest from both practitioners and theoreticians.</p>

<p>I studied statistical machine learning theory (SML) in school, and the classical thinking would suggest deep learning is actually <em>anti-interesting</em>. That is, there is nothing that immediately stands out about deep learning that makes it more desirable than other function approximation methods, and in fact, there are some qualities that make it problematic, such training involving high-dimensional non-convex optimization.</p>

<p>After school when I became a machine learning practitioner, I developed an appreciation and acknowledgement that something special was going on. Deep learning models for natural language applications was a critical component of our product at <a href="http://x.ai">x.ai</a>. But I was basically riding on the huge wave of institutional knowledge formed for deep learning for NLP tasks, and didn’t have good intuition about when it worked better than other approaches or why.</p>

<p><img src="/assets/uc_theory_practice.jpg" alt="shirt" height="50%" width="50%" /></p>

<h3 id="generalization-error">Generalization Error</h3>

<p>First I’ll explain the classical statistics take on deep learning.</p>

<p>Suppose we have <script type="math/tex">n</script> i.i.d. realizations of the pair <script type="math/tex">(\mathbf{x_i},y_i)</script>, where <script type="math/tex">\mathbf{x_i}</script> is a <script type="math/tex">D</script>-dimensional real-valued vector on the unit cube <script type="math/tex">[0,1]^D</script>, and <script type="math/tex">y_i</script> is generated by</p>

<p>\[y_i = f(x_i) + \epsilon_i, \]</p>

<p><script type="math/tex">\epsilon_i</script> is Gaussian noise and <script type="math/tex">f</script> is a a function <script type="math/tex">f: [0,1]^D\rightarrow \mathbb{R}</script>. For some estimate <script type="math/tex">\hat{f}</script> of <script type="math/tex">f</script>, we measure the its error using the integrated squared norm (with respect to the probability measure of <script type="math/tex">x</script>, denoted by <script type="math/tex">P</script>.)</p>

<p>\[ R(\hat{f},f) = \left( \intop_\mathbb{R} \mid \hat{f}(x) - f(x) \mid ^2 dP(x) \right)^{1/2} \]</p>

<p>To have any hope to learn <script type="math/tex">f</script> we need to make some assumptions on its structure. In the classical nonparametric literature, you put some constraints on the smoothness of <script type="math/tex">f</script>, such that it belongs to a Hölder class. A function belongs to a <script type="math/tex">\beta</script>-Hölder class <script type="math/tex">\mathcal{H}_\beta</script> if it has continuous derivatives up to order <script type="math/tex">\beta</script>, and the <script type="math/tex">\beta</script>-th derivative is <strong>Hölder continuous</strong>. If we let <script type="math/tex">\beta</script> be a positive integer, Hölder continuity just means that the function is bounded over it’s domain. For example, the function <script type="math/tex">sin(1/x)</script> has a second derivative which goes to infinity as <script type="math/tex">x</script> approaches 0, so it isn’t Hölder.</p>

<p><img src="/assets/relu/functions.png" alt="functions" height="75%" width="75%" /></p>

<p><em>Left: A function which rapidly oscillates near 0. Right: A Hölder Function.</em></p>

<p>Now, we may ask, as the number of data samples increase, how much better should we expect an estimator to get? For a worst-case analysis, this is answered by the minimax bound, which says that for some constant <script type="math/tex">c>0</script>, as the sample size grows <script type="math/tex">n\rightarrow \infty</script>,</p>

<p>\[ \inf_{\hat{f}} \sup_{f\in H_\beta}\left\{R(\hat{f},f)\right\} &gt; c n^{-2\beta/(2\beta + D)}. \]</p>

<p>This says that for any estimator <script type="math/tex">\hat{f}</script>, we can find a function <script type="math/tex">f \in \mathcal{H_{\beta}}</script> that has error higher than the lower bound. There are two important implications of this.</p>

<ol>
  <li>
    <p>The infimum is over any estimator. So the lower bound applies to any neural net estimator, any kernel estimator, etc.</p>
  </li>
  <li>
    <p>The lower bound is pretty bad. To keep the error small, we need the number of samples to be an exponential function of the feature size <script type="math/tex">D</script>. Keep in mind that for applications of deep learning, the input space is at least in the thousands. This is the <strong>statistical curse of dimensionality</strong>.</p>
  </li>
</ol>

<p>It’s also important to know that many well-known estimators can achieve this lower bound. By that I mean, the minimax bound says no estimate can have error smaller than <script type="math/tex">O\left(n^\frac{-2\beta}{2\beta+D}\right)</script>, but you can also prove for many methods, including kernel methods, splines, and even neural nets, the <em>upper-bound</em> on the error is also <script type="math/tex">O\left(n^\frac{-2\beta}{2\beta+D}\right)</script>.</p>

<h3 id="non-smooth-function-estimation">Non-smooth Function Estimation</h3>

<p>It turns out that the reason deep learning is interesting from a SML perspective is they are good at learning <em>non-smooth</em> functions, particularly functions that are piecewise smooth, explained by <a href="https://arxiv.org/pdf/1802.04474.pdf">this paper</a>:</p>

<blockquote>
  <p>ReLU functions can approximate step functions, and a composition of the step functions in
a combination of other parts of the network can easily express smooth functions restricted
to pieces. In contrast, even though the other methods have the universal approximation
property, they require a larger number of parameters to approximate non-smooth structures.</p>
</blockquote>

<p><img src="/assets/relu/nonsmooth.png" alt="functions" height="75%" width="75%" /></p>

<p><em>Left: Combination of sigmoid functions can approximate a step function. Right: Combination of ReLU functions can approximate a step function.</em></p>

<p>Going through the results of the paper would send us down a pretty deep hole of non-smooth function spaces etc. But here is the key part of the paper. Consider the family of linear estimators, which take the form</p>

<p>\[ \hat{f}_{lin}(x) = \sum \Psi_i(x,\mathbf{x_i})y_i. \]</p>

<p>This includes the most common nonparametric approaches, like splines, kernel estimator, and Gaussian processes, but not deep learning. Every linear estimator <strong>can not achieve the minimax lower bound</strong> if we assume the true function <script type="math/tex">f</script> is piecewise smooth, whereas deep learning achieves the optimal rate. (For that to be possible, the number of weights and layers for the neural net needs to increase with the number of samples at a particular rate described in the paper.)</p>

<p>The downside of this paper is it still has a <em>curse of dimensionality</em> – the number of samples needed to keep the error small grows exponentially with <script type="math/tex">D</script>. But there are other results in the literature that suggest some methods are adaptive to the <a href="http://papers.nips.cc/paper/2577-maximum-likelihood-estimation-of-intrinsic-dimension.pdf">intrinsic dimension</a>. This means that the features live on some lower dimensional manifold of dimension <script type="math/tex">% <![CDATA[
d<<D %]]></script>, the estimator’s convergence only depends on <script type="math/tex">d</script> rather than <script type="math/tex">D</script>. There are ways of reasoning out of the curse of dimensionality.</p>

<p>Pairing these ideas together, you can start to form a picture of the unreasonable effectiveness of deep learning.</p>

<h3 id="the-nonconvexity-problem">The nonconvexity problem</h3>
<p><img src="/assets/nonconvex.png" alt="nonconvex" height="60%" width="60%" /></p>

<p>We haven’t yet gotten to the second complaint about deep learning, which is computation.  To train a deep learning model, one needs to solve a gnarly, non-convex optimization problem.  Generally speaking, when you try to solve a high-dimensional non-convex problem, you are at great risk of getting stuck in the basin of attraction of a local optimum. The local optimum could be much worse than the global optimum. There are a lot of techniques for “jumping out” of the basin of attraction, but in deep learning it’s hard enough just to find a local minimum, doing further exploration is not plausible.</p>

<p>The computational puzzle of deep learning is: how is it that local minima are almost always good solutions?</p>

<p>There have been a few papers in recent years tackling this question, such as <a href="https://icml.cc/Conferences/2018/Schedule?showEvent=2780">Essentially No Barriers in Neural Network Energy Landscape</a> by Draxler et. al. The gist is that local minima tend to be pretty good for neural nets – either local minima are also global minima, or they are essentially just as good as the global minimum.</p>

<h3 id="further-reading">Further reading</h3>

<p>For a collection of high-quality work to better understand deep learning, check out <a href="https://stats385.github.io/">this</a> course, organized by David Donoho.</p>

  </div><a class="u-url" href="/2020/01/25/ReLU/" hidden></a>
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy;  - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="http://localhost:4000/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>

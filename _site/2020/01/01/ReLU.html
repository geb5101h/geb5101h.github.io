<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Deep Learning- It works in practice, but does it work in theory? | Eric Janofsky</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Deep Learning- It works in practice, but does it work in theory?" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/2020/01/01/ReLU.html" />
<meta property="og:url" content="http://localhost:4000/2020/01/01/ReLU.html" />
<meta property="og:site_name" content="Eric Janofsky" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-01-01T16:52:42-05:00" />
<script type="application/ld+json">
{"datePublished":"2020-01-01T16:52:42-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2020/01/01/ReLU.html"},"url":"http://localhost:4000/2020/01/01/ReLU.html","headline":"Deep Learning- It works in practice, but does it work in theory?","dateModified":"2020-01-01T16:52:42-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Eric Janofsky" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Eric Janofsky</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about.html">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><img src="/assets/uc_theory_practice.jpg" alt="My helpful screenshot" /></p>

<p>For a long time I was not impressed with deep learning. In graduate school I was pretty entrenched in the statistical machine learning school of thought. Plus, Chicago is a place where people walk around wearing shirts that say “it works in practice, but does it work in theory?”.</p>

<p>My ML bible is still <a href="https://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576/ref=sr_1_2?keywords=statistical+learning&amp;qid=1579795046&amp;sr=8-2">Elements of Statistical Learning</a>.</p>

<p>From the classical SML perspective, deep learning is not that impressive; in fact, it is problematic both from the statistical and computational perspective.</p>

<h2 id="generalization-error">Generalization Error</h2>

<p>Suppose we have $n$ i.i.d. realizations of the pair <script type="math/tex">(x_i,y_i)</script>, where <script type="math/tex">x_i</script> is a <script type="math/tex">D</script>-dimensional real-valued vector <script type="math/tex">y_i</script> is generated by</p>

<p>\[y_i = f(x_i) + \epsilon_i \]
\[ x \leq O(n^{2\beta/(2\beta + D)}), \]</p>

<p><script type="math/tex">\epsilon_i</script> is Gaussian noise and <script type="math/tex">f</script> is a <script type="math/tex">D</script> a function <script type="math/tex">f: \mathbb{R}^D\rightarrow \mathbb{R}</script>. To have any hope to learn <script type="math/tex">f</script> we need to make some assumptions on its structure. In the classical nonparametric literature, you put some constraints on the smoothness of <script type="math/tex">f</script>, such that it belongs to a Holder class ()</p>

<p>This is actually a bad result. Say we want to know how many samples it would take to learn a function with two continuous derivatives, bounding the estimation error to <script type="math/tex">\kappa</script>. The number of samples required is</p>

<p>\[ n = \kappa^{-1-D/4}. \]</p>

<p>The number of required samples grows <strong>exponentially</strong> with the number of features! In applications where the number of inputs is in the thousands, this means our generalization error in the minimax sense will be pretty bad.</p>

<p>Statistically</p>

<p>Up to today, there is a big gap between theoretical and applied research progress for deep learning. This is a deviation from past trends in ML, such as <a href="https://dl.acm.org/doi/book/10.5555/559923">kernel methods</a>, which has deep roots in theory, and has enjoyed interest from both practitioners and theoreticians.</p>

<p>Deep learning models for natural language applications was a critical component of our product at <a href="http://x.ai">x.ai</a></p>

<p>I went to a presentation at JSM this summer by one of the co-authors of <a href="https://arxiv.org/pdf/1802.04474.pdf">this</a> paper. For</p>

<blockquote>
  <p>ReLU functions can approximate step functions, and a composition of the step functions in
a combination of other parts of the network can easily express smooth functions restricted
to pieces. In contrast, even though the other methods have the universal approximation
property, they require a larger number of parameters to approximate non-smooth structures.</p>
</blockquote>

<p>markdown this is a tests</p>

<h3 id="the-nonconvexity-problem">The nonconvexity problem</h3>
<p><img src="/assets/nonconvex.png" alt="nonconvex" /></p>

<p>The second complaint about DL still stands. One still needs to solve a gnarly, non-convex optimization problem to get the above guarantees. Generally speaking, when you try to solve a non-convex problem, you are at great risk of getting stuck in the domain of attraction of a local optimum. The local optimum could be much worse than the global optimum. There are a lot of techniques for “jumping out”</p>

<p>There have been a few papers in recent years tackling this question, such as <a href="https://icml.cc/Conferences/2018/Schedule?showEvent=2780">Essentially No Barriers in Neural Network Energy Landscape</a> by Draxler et. al. The gist is that local minima tend to be pretty good for neural nets – either local minima are also global minima, or they are essentially just as good as the global minimum.</p>

  </div><a class="u-url" href="/2020/01/01/ReLU.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Eric Janofsky</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Eric Janofsky</li><li><a class="u-email" href="mailto:ebjanofsky@gmail.com">ebjanofsky@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/geb5101h"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">geb5101h</span></a></li><li><a href="https://www.twitter.com/geb5101h"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">geb5101h</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>

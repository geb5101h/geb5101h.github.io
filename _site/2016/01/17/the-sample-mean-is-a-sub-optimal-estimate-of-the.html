<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>The sample mean is a sub-optimal estimate of the population mean | Eric Janofsky</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="The sample mean is a sub-optimal estimate of the population mean" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="It may be surprising that the sample mean is not usually the best estimate of the population mean. It’s well known that when the data is Gaussian, the sample mean is the UMVUE, or the uniformly minimum variance unbiased estimator. The UMVUE has the smallest variance among all unbiased estimators for any choice of the unknown parameter \( \theta \).  The sample mean is always unbiased (when the population mean exists) but isn’t generally minimum variance, and in some cases can have a much higher variance than optimal. This general result was first discovered by Kagan, Linnik and Rao (1965). This derivation comes from Székely et al., 1986. Let \(F\) be a mean-zero, finite variance distribution function, and let \(F_{\theta}(x) = F(x-\theta)\). \(\theta\) is known as the location parameter, and in this setup it is the same as the mean of the distribution. We denote the density \( f(x) = F(x)’\). An Example Suppose that the base distribution is \(\text{Uniform(-1,1)}\), so that \( f(x) = \frac{1}{2} 1 \left\{x\in\{-1,1\}\right\}\). We have \(N\) independent and identically distributed samples from the distribution \(X_1,\ldots,X_N\). Consider the estimator \( X^* = \frac{1}{2}(X_{(1)}+X_{(N)}),\) that is, the average of the smallest and largest point in the sample. This is called the midrange. \(X^*\) may be shown to have variance \( \text{var}(X^*) = \frac{2}{(N+1)(N+2)}, \) while the sample mean \( \bar{X} \) has variance \( \text{var}(\bar{X}) = \frac{1}{3N}, \) which is substantially larger. \(X^*\) has variance of order \( O(N^{-2}) \), which is a whole order of magnitude smaller than that of the sample mean. The main result We will now construct a statistic which is asymptotically UMVUE: it is asymptotally unbiased and minimum variance among all estimators of the population mean for data arising from a particular location family. Consider the estimator \( X^* = \sum_{i=1}^N a_{i,N} X_{(i)}, \) where \(X_{(i)}\) is the i-th order statistic of the sample, and \(a_{i,N}\) are weights which depend on the distribution. Suppose the distribution function \(F\) has three derivatives, let \(f(x) = F(x)’\),  and define the functional \[ a(F(x)) = - \left(\left( A + Bx \right)\left(\log f(x)\right)’ \right)’, \] where \(\begin{align} A &amp;= \frac{\mu_2}{\mu_0\mu_2 - \mu_1^2},  &amp; B = \frac{\mu_1}{\mu_0\mu_2 - \mu_1^2},  \end{align}\) and  \( \begin{align} \mu_0 &amp;= \intop \frac{f’(x)^2}{f(x)} dx,   &amp; \mu_1 = \intop x\frac{f’(x)^2}{f(x)} dx,\end{align}\) \(  \mu_2 = \intop x^2\frac{f’(x)^2}{f(x)} dx - 1, \) then with the choice \(a_{i,N} = a(i/N)/N \), \(X^*\) is asymptotically UMVUE." />
<meta property="og:description" content="It may be surprising that the sample mean is not usually the best estimate of the population mean. It’s well known that when the data is Gaussian, the sample mean is the UMVUE, or the uniformly minimum variance unbiased estimator. The UMVUE has the smallest variance among all unbiased estimators for any choice of the unknown parameter \( \theta \).  The sample mean is always unbiased (when the population mean exists) but isn’t generally minimum variance, and in some cases can have a much higher variance than optimal. This general result was first discovered by Kagan, Linnik and Rao (1965). This derivation comes from Székely et al., 1986. Let \(F\) be a mean-zero, finite variance distribution function, and let \(F_{\theta}(x) = F(x-\theta)\). \(\theta\) is known as the location parameter, and in this setup it is the same as the mean of the distribution. We denote the density \( f(x) = F(x)’\). An Example Suppose that the base distribution is \(\text{Uniform(-1,1)}\), so that \( f(x) = \frac{1}{2} 1 \left\{x\in\{-1,1\}\right\}\). We have \(N\) independent and identically distributed samples from the distribution \(X_1,\ldots,X_N\). Consider the estimator \( X^* = \frac{1}{2}(X_{(1)}+X_{(N)}),\) that is, the average of the smallest and largest point in the sample. This is called the midrange. \(X^*\) may be shown to have variance \( \text{var}(X^*) = \frac{2}{(N+1)(N+2)}, \) while the sample mean \( \bar{X} \) has variance \( \text{var}(\bar{X}) = \frac{1}{3N}, \) which is substantially larger. \(X^*\) has variance of order \( O(N^{-2}) \), which is a whole order of magnitude smaller than that of the sample mean. The main result We will now construct a statistic which is asymptotically UMVUE: it is asymptotally unbiased and minimum variance among all estimators of the population mean for data arising from a particular location family. Consider the estimator \( X^* = \sum_{i=1}^N a_{i,N} X_{(i)}, \) where \(X_{(i)}\) is the i-th order statistic of the sample, and \(a_{i,N}\) are weights which depend on the distribution. Suppose the distribution function \(F\) has three derivatives, let \(f(x) = F(x)’\),  and define the functional \[ a(F(x)) = - \left(\left( A + Bx \right)\left(\log f(x)\right)’ \right)’, \] where \(\begin{align} A &amp;= \frac{\mu_2}{\mu_0\mu_2 - \mu_1^2},  &amp; B = \frac{\mu_1}{\mu_0\mu_2 - \mu_1^2},  \end{align}\) and  \( \begin{align} \mu_0 &amp;= \intop \frac{f’(x)^2}{f(x)} dx,   &amp; \mu_1 = \intop x\frac{f’(x)^2}{f(x)} dx,\end{align}\) \(  \mu_2 = \intop x^2\frac{f’(x)^2}{f(x)} dx - 1, \) then with the choice \(a_{i,N} = a(i/N)/N \), \(X^*\) is asymptotically UMVUE." />
<link rel="canonical" href="http://localhost:4000/2016/01/17/the-sample-mean-is-a-sub-optimal-estimate-of-the.html" />
<meta property="og:url" content="http://localhost:4000/2016/01/17/the-sample-mean-is-a-sub-optimal-estimate-of-the.html" />
<meta property="og:site_name" content="Eric Janofsky" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2016-01-17T09:31:25-05:00" />
<script type="application/ld+json">
{"description":"It may be surprising that the sample mean is not usually the best estimate of the population mean. It’s well known that when the data is Gaussian, the sample mean is the UMVUE, or the uniformly minimum variance unbiased estimator. The UMVUE has the smallest variance among all unbiased estimators for any choice of the unknown parameter \\( \\theta \\).  The sample mean is always unbiased (when the population mean exists) but isn’t generally minimum variance, and in some cases can have a much higher variance than optimal. This general result was first discovered by Kagan, Linnik and Rao (1965). This derivation comes from Székely et al., 1986. Let \\(F\\) be a mean-zero, finite variance distribution function, and let \\(F_{\\theta}(x) = F(x-\\theta)\\). \\(\\theta\\) is known as the location parameter, and in this setup it is the same as the mean of the distribution. We denote the density \\( f(x) = F(x)’\\). An Example Suppose that the base distribution is \\(\\text{Uniform(-1,1)}\\), so that \\( f(x) = \\frac{1}{2} 1 \\left\\{x\\in\\{-1,1\\}\\right\\}\\). We have \\(N\\) independent and identically distributed samples from the distribution \\(X_1,\\ldots,X_N\\). Consider the estimator \\( X^* = \\frac{1}{2}(X_{(1)}+X_{(N)}),\\) that is, the average of the smallest and largest point in the sample. This is called the midrange. \\(X^*\\) may be shown to have variance \\( \\text{var}(X^*) = \\frac{2}{(N+1)(N+2)}, \\) while the sample mean \\( \\bar{X} \\) has variance \\( \\text{var}(\\bar{X}) = \\frac{1}{3N}, \\) which is substantially larger. \\(X^*\\) has variance of order \\( O(N^{-2}) \\), which is a whole order of magnitude smaller than that of the sample mean. The main result We will now construct a statistic which is asymptotically UMVUE: it is asymptotally unbiased and minimum variance among all estimators of the population mean for data arising from a particular location family. Consider the estimator \\( X^* = \\sum_{i=1}^N a_{i,N} X_{(i)}, \\) where \\(X_{(i)}\\) is the i-th order statistic of the sample, and \\(a_{i,N}\\) are weights which depend on the distribution. Suppose the distribution function \\(F\\) has three derivatives, let \\(f(x) = F(x)’\\),  and define the functional \\[ a(F(x)) = - \\left(\\left( A + Bx \\right)\\left(\\log f(x)\\right)’ \\right)’, \\] where \\(\\begin{align} A &amp;= \\frac{\\mu_2}{\\mu_0\\mu_2 - \\mu_1^2},  &amp; B = \\frac{\\mu_1}{\\mu_0\\mu_2 - \\mu_1^2},  \\end{align}\\) and  \\( \\begin{align} \\mu_0 &amp;= \\intop \\frac{f’(x)^2}{f(x)} dx,   &amp; \\mu_1 = \\intop x\\frac{f’(x)^2}{f(x)} dx,\\end{align}\\) \\(  \\mu_2 = \\intop x^2\\frac{f’(x)^2}{f(x)} dx - 1, \\) then with the choice \\(a_{i,N} = a(i/N)/N \\), \\(X^*\\) is asymptotically UMVUE.","datePublished":"2016-01-17T09:31:25-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2016/01/17/the-sample-mean-is-a-sub-optimal-estimate-of-the.html"},"url":"http://localhost:4000/2016/01/17/the-sample-mean-is-a-sub-optimal-estimate-of-the.html","headline":"The sample mean is a sub-optimal estimate of the population mean","dateModified":"2016-01-17T09:31:25-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Eric Janofsky" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Eric Janofsky</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about.html">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>It may be surprising that the sample mean is not usually the best estimate of the population mean. It’s well known that when the data is Gaussian, the sample mean is the <i>UMVUE</i>, or the uniformly minimum variance unbiased estimator. The UMVUE has the smallest variance among all unbiased estimators for <i>any </i>choice of the unknown parameter \( \theta \).  The sample mean is always unbiased (when the population mean exists) but isn’t generally minimum variance, and in some cases can have a much higher variance than optimal. This general result was first discovered by Kagan, Linnik and Rao (1965). This derivation comes from Székely et al., 1986.</p><p>Let \(F\) be a mean-zero, finite variance distribution function, and let \(F_{\theta}(x) = F(x-\theta)\). \(\theta\) is known as the <i>location parameter, </i>and in this setup it is the same as the mean of the distribution. We denote the density \( f(x) = F(x)’\).</p><p><b>An Example</b></p><p>Suppose that the base distribution is \(\text{Uniform(-1,1)}\), so that \( f(x) = \frac{1}{2} 1 \left\{x\in\{-1,1\}\right\}\). We have \(N\) independent and identically distributed samples from the distribution \(X_1,\ldots,X_N\). Consider the estimator</p><p>\( X^* = \frac{1}{2}(X_{(1)}+X_{(N)}),\)</p><p>that is, the average of the smallest and largest point in the sample. This is called the <i>midrange</i>. \(X^*\) may be shown to have variance</p><p>\( \text{var}(X^*) = \frac{2}{(N+1)(N+2)}, \)</p><p>while the sample mean \( \bar{X} \) has variance</p><p>\( \text{var}(\bar{X}) = \frac{1}{3N}, \)</p><p>which is substantially larger. \(X^*\) has variance of order \( O(N^{-2}) \), which is a whole order of magnitude smaller than that of the sample mean.</p><p><b>The main result</b></p><p>We will now construct a statistic which is <i>asymptotically UMVUE</i>: it is asymptotally unbiased and minimum variance among all estimators of the population mean for data arising from a particular location family.</p><p>Consider the estimator</p><p>\( X^* = \sum_{i=1}^N a_{i,N} X_{(i)}, \)</p><p>where \(X_{(i)}\) is the i-th order statistic of the sample, and \(a_{i,N}\) are weights which depend on the distribution.</p><p>Suppose the distribution function \(F\) has three derivatives, let \(f(x) = F(x)’\),  and define the functional<br/></p><p>\[ a(F(x)) = - \left(\left( A + Bx \right)\left(\log f(x)\right)’ \right)’, \]</p><p>where</p><p>\(\begin{align} A &amp;= \frac{\mu_2}{\mu_0\mu_2 - \mu_1^2},  &amp; B = \frac{\mu_1}{\mu_0\mu_2 - \mu_1^2},  \end{align}\)</p><p>and </p><p>\( \begin{align} \mu_0 &amp;= \intop \frac{f’(x)^2}{f(x)} dx,   &amp; \mu_1 = \intop x\frac{f’(x)^2}{f(x)} dx,\end{align}\)<br/></p><p>\(  \mu_2 = \intop x^2\frac{f’(x)^2}{f(x)} dx - 1, \)</p><p>then with the choice \(a_{i,N} = a(i/N)/N \), \(X^*\) is asymptotically UMVUE.</p>

  </div><a class="u-url" href="/2016/01/17/the-sample-mean-is-a-sub-optimal-estimate-of-the.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Eric Janofsky</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Eric Janofsky</li><li><a class="u-email" href="mailto:ebjanofsky@gmail.com">ebjanofsky@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/geb5101h"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">geb5101h</span></a></li><li><a href="https://www.twitter.com/geb5101h"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">geb5101h</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>

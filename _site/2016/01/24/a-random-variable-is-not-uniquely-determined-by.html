<p>The <i>moments </i>of a random variable \(X\) are given by \(\mathbb{E}[X^n]\), for all integers \(n\geq 1\). One fascinating fact I first learned while studying distribution theory is that the moment sequence does not always uniquely determine a distribution.</p><p>Consider \(X\sim logNormal(0,1)\), that is, \(log(X)\) follows a standard normal distribution, and let the density of \(X\) be \(f(x)\). The moments of \(X\) exist and have the closed form</p><p>\[ m_i := \mathbb{E}[X^i] = e^{i^2/2}. \]</p><p>Consider the density</p><p>\[ f_a(x) = f(x)(1+\sin(2\pi\log(x))), \,\,\, x\geq 0. \]</p><p>Then \(f,f_a\) have the same moments. To prove this, itâ€™s sufficient to show that for each \(i=1,2,\ldots\),</p><p>\[ \intop_{0}^\infty x^i f(x) \sin(2\pi\log(x)) dx =0 .\]</p><p>This can be verified by applying the change-of-variables \(y=\log x\) to the integral and showing the resulting integral is over an odd function on the real line.</p><figure data-orig-width="534" data-orig-height="443" class="tmblr-full"><img src="https://66.media.tumblr.com/b4bf1c6bdbb8c9b3cd2d59362a35061f/tumblr_inline_o1grspsXys1tlyjch_540.png" alt="image" data-orig-width="534" data-orig-height="443"/></figure><p>Figure 1: Two densities with the same moments</p><p><b>When do the moments determine the distribution?</b></p><p>One sufficient condition for moments uniquely determining the distribution is the <i>Carleman condition</i>:</p><p>\[ \sum_{i=1}^\infty m_{2i}^{-1/(2i)} = \infty. \]</p><p>You can verify that the lognormal distribution has moments which grow too quickly for this Condition to hold.</p>

<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Pitman Closeness, a strange alternative to risk | Eric Janofsky</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Pitman Closeness, a strange alternative to risk" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Here is the curious story of an alternative to the accepted notions of statistical optimality, which makes a lot of sense conceptually but can lead you to some bizarre conclusions. Statistical decision theory begins by considering an observation \(x\) drawn from a distribution \(F(x\mid \theta)\) parametrized by \(\theta\), a decision rule \(\delta\) which is a measurable function of the data \(x\), and a loss function \(L(\theta,\delta(x))\), which measures the loss from taking some action \(\delta\). The risk is defined as \[ R(\theta,\delta) = \mathbb{E}_F[ L(\theta,\delta(X))], \] which measures the expected loss averaging over the distribution \(F\). Decision-theoretic concepts of optimality are defined with respect to the risk. For example, a decision rule \(\delta^*\) is minimax if it minimizes the maximum risk over a class of decision rules \(\mathcal{D}\): for all \(\delta\in\mathcal{D}\), \[ \max_\theta R(\theta,\delta^*) \leq \max_\theta R(\theta,\delta). \] In general there could be multiple minimax decision rules. Another desirable property of a decision rule \(\delta^*\) is admissability, which says that there is no decision rule \(\delta\) which dominates \(\delta_1\). A decision rule \(\delta\) dominates \(\delta_1\) if \[ R(\theta,\delta) \leq R(\theta,\delta^*) \] for all \(\theta\), with strict inequality for some \(\theta\). Admissability is a desirable but not sufficient measure of optimality. For example a constant estimator is usually admissable (it has the minimal possible risk when the parameter takes its value). An admissable rule need not be minimax, nor a minimax rule admissable. Thus if a decision rule is both admissable and minimax it should be put in high regard. Pitman Closeness An interesting alternative to comparing estimators according to risk was proposed in Pitman, 1937. A decision rule \(\delta_1\) Pitman dominates \(\delta_2\), denoted \(\delta_1 \overset{P}{\succ}\delta_2\) if for all \(\theta\), \[ P(L(\theta,\delta_1) \leq L(\theta,\delta_2)) &gt; &amp;frac12;. \] Pitman domination simply says that it is more probable than not that one decision rule has smaller loss than the other. This criterion appears to have some advantages over risk. Firstly, it considers the entire distribution of the loss \(L(\theta,\delta)\), while the risk is just the expectation over the loss. Also crucially, it involves the joint distribution of the pair \(\{L(\theta,\delta_1),L(\theta,\delta_2)\}\). At first glance, this looks like a good way to compare decision rules. A decision rule which Pitman dominates the normal sample mean Consider an i.i.d. sample \(X_1,\ldots,X_N\) from a univariate normal distribution, \(Normal(\theta,1)\). It is well known that the sample mean \(\bar{X}\) is unbiased, UMVUE, minimax and admissable. Weirdly enough, there is an estimator which Pitman dominates the sample mean. This example comes from Efron, 1975. Define \(X^*\) by \[ X^* = \bar{X} - \Delta(\bar{X}), \] where \(\Delta\) is an odd function, which for \(x\geq 0\) takes the values, \[ \Delta(x) = \frac{1}{2\sqrt{N}} \min \left\{\sqrt{N}x, \Phi(-\sqrt{N}x)\right\}. \] Then \(X^* \overset{P}{\succ} \bar{X}\). What’s going on? Observe that \(X^*\) is a function of \(\bar{X}\) which shrinks it towards zero. This sounds unintuitive, since the rule dominates the sample mean for any \(\theta\), even when it is very far from 0! But there is actually a similar phenomenon in decision theory, which is known as the Stein effect, in which you may construct an estimator which dominates a minimax estimator by shrinking it towards zero. In a monumental work, Charles Stein proved that this Stein effect occurs when estimating a normal mean vector of length at least 3, when your loss function is the squared-error loss. But no estimator dominates the sample mean in the univariate scenario. The peculiar thing about Pitman domination is you get a Stein-type effect even for estimation of a single normal mean. Even in the simplest possible estimation problem, we end up with a paradoxical result. Pitman Closeness is not transitive The next strange property of Pitman closeness is that it is not transitive. In general, it can’t formulate an ordering over a set of decision rules, and as a consequence there may be no Pitman dominant decision rule. This example comes from Robert, 2007. Let \(x \sim Uniform(-0.9\theta,1.1\theta)\). Then consider the decision rules \(\delta_0(x) = x,\delta_1(x)=0.9\mid x \mid\), and \(\delta_2(x) = 3.2 \mid x \mid \). Then \(\delta_0 \overset{P}{\succ}\delta_1,d_1 \overset{P}{\succ}\delta_2,\) and \(d_2 \overset{P}{\succ}\delta_0\). Conclusion Pitman closeness is an interesting idea, and was the focus of a fair amount of research at one time. There was a book devoted to it. But today it is a little-known curiosity. Pitman closeness begins with eminently reasonable foundations but then leads to too many conclusions that go against what we might want from a reasonable decision theory. The risk approach to decision theory, despite some it’s own paradoxes, is the accepted benchmark in statisics." />
<meta property="og:description" content="Here is the curious story of an alternative to the accepted notions of statistical optimality, which makes a lot of sense conceptually but can lead you to some bizarre conclusions. Statistical decision theory begins by considering an observation \(x\) drawn from a distribution \(F(x\mid \theta)\) parametrized by \(\theta\), a decision rule \(\delta\) which is a measurable function of the data \(x\), and a loss function \(L(\theta,\delta(x))\), which measures the loss from taking some action \(\delta\). The risk is defined as \[ R(\theta,\delta) = \mathbb{E}_F[ L(\theta,\delta(X))], \] which measures the expected loss averaging over the distribution \(F\). Decision-theoretic concepts of optimality are defined with respect to the risk. For example, a decision rule \(\delta^*\) is minimax if it minimizes the maximum risk over a class of decision rules \(\mathcal{D}\): for all \(\delta\in\mathcal{D}\), \[ \max_\theta R(\theta,\delta^*) \leq \max_\theta R(\theta,\delta). \] In general there could be multiple minimax decision rules. Another desirable property of a decision rule \(\delta^*\) is admissability, which says that there is no decision rule \(\delta\) which dominates \(\delta_1\). A decision rule \(\delta\) dominates \(\delta_1\) if \[ R(\theta,\delta) \leq R(\theta,\delta^*) \] for all \(\theta\), with strict inequality for some \(\theta\). Admissability is a desirable but not sufficient measure of optimality. For example a constant estimator is usually admissable (it has the minimal possible risk when the parameter takes its value). An admissable rule need not be minimax, nor a minimax rule admissable. Thus if a decision rule is both admissable and minimax it should be put in high regard. Pitman Closeness An interesting alternative to comparing estimators according to risk was proposed in Pitman, 1937. A decision rule \(\delta_1\) Pitman dominates \(\delta_2\), denoted \(\delta_1 \overset{P}{\succ}\delta_2\) if for all \(\theta\), \[ P(L(\theta,\delta_1) \leq L(\theta,\delta_2)) &gt; &amp;frac12;. \] Pitman domination simply says that it is more probable than not that one decision rule has smaller loss than the other. This criterion appears to have some advantages over risk. Firstly, it considers the entire distribution of the loss \(L(\theta,\delta)\), while the risk is just the expectation over the loss. Also crucially, it involves the joint distribution of the pair \(\{L(\theta,\delta_1),L(\theta,\delta_2)\}\). At first glance, this looks like a good way to compare decision rules. A decision rule which Pitman dominates the normal sample mean Consider an i.i.d. sample \(X_1,\ldots,X_N\) from a univariate normal distribution, \(Normal(\theta,1)\). It is well known that the sample mean \(\bar{X}\) is unbiased, UMVUE, minimax and admissable. Weirdly enough, there is an estimator which Pitman dominates the sample mean. This example comes from Efron, 1975. Define \(X^*\) by \[ X^* = \bar{X} - \Delta(\bar{X}), \] where \(\Delta\) is an odd function, which for \(x\geq 0\) takes the values, \[ \Delta(x) = \frac{1}{2\sqrt{N}} \min \left\{\sqrt{N}x, \Phi(-\sqrt{N}x)\right\}. \] Then \(X^* \overset{P}{\succ} \bar{X}\). What’s going on? Observe that \(X^*\) is a function of \(\bar{X}\) which shrinks it towards zero. This sounds unintuitive, since the rule dominates the sample mean for any \(\theta\), even when it is very far from 0! But there is actually a similar phenomenon in decision theory, which is known as the Stein effect, in which you may construct an estimator which dominates a minimax estimator by shrinking it towards zero. In a monumental work, Charles Stein proved that this Stein effect occurs when estimating a normal mean vector of length at least 3, when your loss function is the squared-error loss. But no estimator dominates the sample mean in the univariate scenario. The peculiar thing about Pitman domination is you get a Stein-type effect even for estimation of a single normal mean. Even in the simplest possible estimation problem, we end up with a paradoxical result. Pitman Closeness is not transitive The next strange property of Pitman closeness is that it is not transitive. In general, it can’t formulate an ordering over a set of decision rules, and as a consequence there may be no Pitman dominant decision rule. This example comes from Robert, 2007. Let \(x \sim Uniform(-0.9\theta,1.1\theta)\). Then consider the decision rules \(\delta_0(x) = x,\delta_1(x)=0.9\mid x \mid\), and \(\delta_2(x) = 3.2 \mid x \mid \). Then \(\delta_0 \overset{P}{\succ}\delta_1,d_1 \overset{P}{\succ}\delta_2,\) and \(d_2 \overset{P}{\succ}\delta_0\). Conclusion Pitman closeness is an interesting idea, and was the focus of a fair amount of research at one time. There was a book devoted to it. But today it is a little-known curiosity. Pitman closeness begins with eminently reasonable foundations but then leads to too many conclusions that go against what we might want from a reasonable decision theory. The risk approach to decision theory, despite some it’s own paradoxes, is the accepted benchmark in statisics." />
<link rel="canonical" href="http://localhost:4000/2016/02/02/pitman-closeness-a-strange-alternative-to-risk.html" />
<meta property="og:url" content="http://localhost:4000/2016/02/02/pitman-closeness-a-strange-alternative-to-risk.html" />
<meta property="og:site_name" content="Eric Janofsky" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2016-02-02T10:01:58-05:00" />
<script type="application/ld+json">
{"description":"Here is the curious story of an alternative to the accepted notions of statistical optimality, which makes a lot of sense conceptually but can lead you to some bizarre conclusions. Statistical decision theory begins by considering an observation \\(x\\) drawn from a distribution \\(F(x\\mid \\theta)\\) parametrized by \\(\\theta\\), a decision rule \\(\\delta\\) which is a measurable function of the data \\(x\\), and a loss function \\(L(\\theta,\\delta(x))\\), which measures the loss from taking some action \\(\\delta\\). The risk is defined as \\[ R(\\theta,\\delta) = \\mathbb{E}_F[ L(\\theta,\\delta(X))], \\] which measures the expected loss averaging over the distribution \\(F\\). Decision-theoretic concepts of optimality are defined with respect to the risk. For example, a decision rule \\(\\delta^*\\) is minimax if it minimizes the maximum risk over a class of decision rules \\(\\mathcal{D}\\): for all \\(\\delta\\in\\mathcal{D}\\), \\[ \\max_\\theta R(\\theta,\\delta^*) \\leq \\max_\\theta R(\\theta,\\delta). \\] In general there could be multiple minimax decision rules. Another desirable property of a decision rule \\(\\delta^*\\) is admissability, which says that there is no decision rule \\(\\delta\\) which dominates \\(\\delta_1\\). A decision rule \\(\\delta\\) dominates \\(\\delta_1\\) if \\[ R(\\theta,\\delta) \\leq R(\\theta,\\delta^*) \\] for all \\(\\theta\\), with strict inequality for some \\(\\theta\\). Admissability is a desirable but not sufficient measure of optimality. For example a constant estimator is usually admissable (it has the minimal possible risk when the parameter takes its value). An admissable rule need not be minimax, nor a minimax rule admissable. Thus if a decision rule is both admissable and minimax it should be put in high regard. Pitman Closeness An interesting alternative to comparing estimators according to risk was proposed in Pitman, 1937. A decision rule \\(\\delta_1\\) Pitman dominates \\(\\delta_2\\), denoted \\(\\delta_1 \\overset{P}{\\succ}\\delta_2\\) if for all \\(\\theta\\), \\[ P(L(\\theta,\\delta_1) \\leq L(\\theta,\\delta_2)) &gt; &amp;frac12;. \\] Pitman domination simply says that it is more probable than not that one decision rule has smaller loss than the other. This criterion appears to have some advantages over risk. Firstly, it considers the entire distribution of the loss \\(L(\\theta,\\delta)\\), while the risk is just the expectation over the loss. Also crucially, it involves the joint distribution of the pair \\(\\{L(\\theta,\\delta_1),L(\\theta,\\delta_2)\\}\\). At first glance, this looks like a good way to compare decision rules. A decision rule which Pitman dominates the normal sample mean Consider an i.i.d. sample \\(X_1,\\ldots,X_N\\) from a univariate normal distribution, \\(Normal(\\theta,1)\\). It is well known that the sample mean \\(\\bar{X}\\) is unbiased, UMVUE, minimax and admissable. Weirdly enough, there is an estimator which Pitman dominates the sample mean. This example comes from Efron, 1975. Define \\(X^*\\) by \\[ X^* = \\bar{X} - \\Delta(\\bar{X}), \\] where \\(\\Delta\\) is an odd function, which for \\(x\\geq 0\\) takes the values, \\[ \\Delta(x) = \\frac{1}{2\\sqrt{N}} \\min \\left\\{\\sqrt{N}x, \\Phi(-\\sqrt{N}x)\\right\\}. \\] Then \\(X^* \\overset{P}{\\succ} \\bar{X}\\). What’s going on? Observe that \\(X^*\\) is a function of \\(\\bar{X}\\) which shrinks it towards zero. This sounds unintuitive, since the rule dominates the sample mean for any \\(\\theta\\), even when it is very far from 0! But there is actually a similar phenomenon in decision theory, which is known as the Stein effect, in which you may construct an estimator which dominates a minimax estimator by shrinking it towards zero. In a monumental work, Charles Stein proved that this Stein effect occurs when estimating a normal mean vector of length at least 3, when your loss function is the squared-error loss. But no estimator dominates the sample mean in the univariate scenario. The peculiar thing about Pitman domination is you get a Stein-type effect even for estimation of a single normal mean. Even in the simplest possible estimation problem, we end up with a paradoxical result. Pitman Closeness is not transitive The next strange property of Pitman closeness is that it is not transitive. In general, it can’t formulate an ordering over a set of decision rules, and as a consequence there may be no Pitman dominant decision rule. This example comes from Robert, 2007. Let \\(x \\sim Uniform(-0.9\\theta,1.1\\theta)\\). Then consider the decision rules \\(\\delta_0(x) = x,\\delta_1(x)=0.9\\mid x \\mid\\), and \\(\\delta_2(x) = 3.2 \\mid x \\mid \\). Then \\(\\delta_0 \\overset{P}{\\succ}\\delta_1,d_1 \\overset{P}{\\succ}\\delta_2,\\) and \\(d_2 \\overset{P}{\\succ}\\delta_0\\). Conclusion Pitman closeness is an interesting idea, and was the focus of a fair amount of research at one time. There was a book devoted to it. But today it is a little-known curiosity. Pitman closeness begins with eminently reasonable foundations but then leads to too many conclusions that go against what we might want from a reasonable decision theory. The risk approach to decision theory, despite some it’s own paradoxes, is the accepted benchmark in statisics.","datePublished":"2016-02-02T10:01:58-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2016/02/02/pitman-closeness-a-strange-alternative-to-risk.html"},"url":"http://localhost:4000/2016/02/02/pitman-closeness-a-strange-alternative-to-risk.html","headline":"Pitman Closeness, a strange alternative to risk","dateModified":"2016-02-02T10:01:58-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Eric Janofsky" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Eric Janofsky</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about.html">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Here is the curious story of an alternative to the accepted notions of statistical optimality, which makes a lot of sense conceptually but can lead you to some bizarre conclusions.</p><p>Statistical decision theory begins by considering an observation \(x\) drawn from a distribution \(F(x\mid \theta)\) parametrized by \(\theta\), a <i>decision rule </i>\(\delta\) which is a measurable function of the data \(x\), and a<i> loss function</i> \(L(\theta,\delta(x))\), which measures the loss from taking some action \(\delta\). The <i>risk </i>is defined as<br/></p><p><i>\[ R(\theta,\delta) = \mathbb{E}_F[ L(\theta,\delta(X))], \]</i></p><p>which measures the expected loss averaging over the distribution \(F\). Decision-theoretic concepts of optimality are defined with respect to the risk. For example, a decision rule \(\delta^*\) is <i>minimax</i> if it minimizes the maximum risk over a class of decision rules \(\mathcal{D}\): for all \(\delta\in\mathcal{D}\),</p><p>\[ \max_\theta R(\theta,\delta^*) \leq \max_\theta R(\theta,\delta). \]</p><p>In general there could be multiple minimax decision rules.</p><p>Another desirable property of a decision rule \(\delta^*\) is <i>admissability</i>, which says that there is no decision rule \(\delta\) which <i>dominates</i> \(\delta_1\). A decision rule \(\delta\) dominates \(\delta_1\) if</p><p>\[ R(\theta,\delta) \leq R(\theta,\delta^*) \]</p><p>for all \(\theta\), with strict inequality for some \(\theta\). Admissability is a desirable but not sufficient measure of optimality. For example a constant estimator is usually admissable (it has the minimal possible risk when the parameter takes its value). An admissable rule need not be minimax, nor a minimax rule admissable. Thus if a decision rule is both admissable and minimax it should be put in high regard.</p><p><b>Pitman Closeness</b><br/></p><p>An interesting alternative to comparing estimators according to risk was proposed in <a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=2028260&amp;fileId=S0305004100019563">Pitman, 1937</a>. A decision rule \(\delta_1\) <i>Pitman dominates </i>\(\delta_2\), denoted \(\delta_1 \overset{P}{\succ}\delta_2\) if for all \(\theta\),</p><p>\[ P(L(\theta,\delta_1) \leq L(\theta,\delta_2)) &gt; &frac12;. \]</p><p>Pitman domination simply says that it is more probable than not that one decision rule has smaller loss than the other. This criterion appears to have some advantages over risk. Firstly, it considers the entire distribution of the loss \(L(\theta,\delta)\), while the risk is just the expectation over the loss. Also crucially, it involves the <i>joint distribution </i>of the pair \(\{L(\theta,\delta_1),L(\theta,\delta_2)\}\). At first glance, this looks like a good way to compare decision rules.</p><!-- more --><p><b>A decision rule which Pitman dominates the normal sample mean</b></p><p>Consider an i.i.d. sample \(X_1,\ldots,X_N\) from a univariate normal distribution, \(Normal(\theta,1)\). It is well known that the sample mean \(\bar{X}\) is unbiased, UMVUE, minimax and admissable. Weirdly enough, there is an estimator which Pitman dominates the sample mean. This example comes from <a href="http://www.sciencedirect.com/science/article/pii/0001870875901140">Efron, 1975</a>.</p><p>Define \(X^*\) by</p><p>\[ X^* = \bar{X} - \Delta(\bar{X}), \]</p><p>where \(\Delta\) is an odd function, which for \(x\geq 0\) takes the values,</p><p>\[ \Delta(x) = \frac{1}{2\sqrt{N}} \min \left\{\sqrt{N}x, \Phi(-\sqrt{N}x)\right\}. \]</p><p>Then \(X^* \overset{P}{\succ} \bar{X}\).</p><p>What’s going on? Observe that \(X^*\) is a function of \(\bar{X}\) which shrinks it towards zero. This sounds unintuitive, since the rule dominates the sample mean for <i>any </i>\(\theta\), even when it is very far from 0! But there is actually a similar phenomenon in decision theory, which is known as the <a href="https://en.wikipedia.org/wiki/Stein%27s_example">Stein effect</a>, in which you may construct an estimator which dominates a minimax estimator by shrinking it towards zero. In a monumental work, Charles Stein proved that this <i>Stein effect</i> occurs when <i>estimating a normal mean</i> <i>vector of length at least 3, </i>when your loss function is the squared-error loss. But no estimator dominates the sample mean in the univariate scenario. The peculiar thing about Pitman domination is you get a Stein-type effect even for estimation of a single normal mean. Even in the simplest possible estimation problem, we end up with a paradoxical result.</p><p><b>Pitman Closeness is not transitive</b></p><p>The next strange property of Pitman closeness is that it is not transitive. In general, it can’t formulate an ordering over a set of decision rules, and as a consequence there may be no Pitman dominant decision rule. This example comes from <a href="https://scholar.google.com/scholar?cluster=14941327657343594317&amp;hl=en&amp;as_sdt=0,14">Robert, 2007</a>.</p><p>Let \(x \sim Uniform(-0.9\theta,1.1\theta)\). Then consider the decision rules \(\delta_0(x) = x,\delta_1(x)=0.9\mid x \mid\), and \(\delta_2(x) = 3.2 \mid x \mid \). Then \(\delta_0 \overset{P}{\succ}\delta_1,d_1 \overset{P}{\succ}\delta_2,\) and \(d_2 \overset{P}{\succ}\delta_0\).</p><p><b>Conclusion</b></p><p>Pitman closeness is an interesting idea, and was the focus of a fair amount of research at one time. There was a <a href="https://scholar.google.com/scholar?cluster=2841823111824779602&amp;hl=en&amp;as_sdt=0,14">book</a> devoted to it. But today it is a little-known curiosity. Pitman closeness begins with eminently reasonable foundations but then leads to too many conclusions that go against what we might want from a reasonable decision theory. The risk approach to decision theory, despite some it’s own paradoxes, is the accepted benchmark in statisics.</p>

  </div><a class="u-url" href="/2016/02/02/pitman-closeness-a-strange-alternative-to-risk.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Eric Janofsky</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Eric Janofsky</li><li><a class="u-email" href="mailto:ebjanofsky@gmail.com">ebjanofsky@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/geb5101h"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">geb5101h</span></a></li><li><a href="https://www.twitter.com/geb5101h"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">geb5101h</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>

<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Eric Janofsky</title>
  <meta name="description" content="">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/">
  
  
  <link rel="alternate" type="application/rss+xml" title="Eric Janofsky" href="http://localhost:4000/feed.xml">

  

  
  <meta property="og:title" content="Eric Janofsky">
  <meta property="og:site_name" content="Eric Janofsky">
  <meta property="og:url" content="http://localhost:4000/">
  <meta property="og:description" content="">
  
  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="geb5101h">
  <meta name="twitter:title" content="Eric Janofsky">
  <meta name="twitter:description" content="">
  
    <meta name="twitter:creator" content="geb5101h">
  
  

  <link rel="dns-prefetch" href="https://fonts.gstatic.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css?family=Bitter:400,400i,700&display=swap" rel="stylesheet">

  
  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-156801543-1', 'auto');
    ga('send', 'pageview');

  </script>


</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Eric Janofsky</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/archives/">Archives</a>
      
        
        <a class="page-link" href="https://github.com/geb5101h">GitHub</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home">

  

  

  <ul class="post-list">
    
      
      

      <li>
        <header class="post-header">
          <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
          </script>
          <h1 class="post-title">
            
              <a class="post-link" href="/2017/09/21/uninformative-priors-say-something-informative/">Uninformative priors say something informative about the posterior predictions</a>
            
          </h1>

          <p class="post-meta">
            Sep 21, 2017
            
              

 •
  
    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        <a href="/tags/bayes/">bayes</a>
      
    
  


            
            
          </p>
        </header>

        <div class="post-content">
          <p>I took a class in Bayesian statistics in year two of graduate school. I wouldn't call myself a Bayesian -- as an industry practitioner, I take the "whatever works" approach. It's done me a lot of good to have a broad toolset, pulling out one tool or another based on what I think is best for the business problem at hand. This tends to be the attitude in ML research, but statistics can still be pretty clan-ny with various "Bayesian" societies and affiliations.</p>


        </div>
        
          <p class="post-continue">
            <a href="/2017/09/21/uninformative-priors-say-something-informative/">Read on &rarr;</a>
          </p>
        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
          </script>
          <h1 class="post-title">
            
              <a class="post-link" href="/2016/03/01/spurious-correlation-unit-roots-and-cointegration/">Spurious correlation, unit roots and cointegration</a>
            
          </h1>

          <p class="post-meta">
            Mar 1, 2016
            
              

 •
  
    
    
      
    
      
    
      
    
      
    
      
    
      
        <a href="/tags/time-series/">time-series</a>
      
    
      
    
  


            
            
          </p>
        </header>

        <div class="post-content">
          <p>I learned about the spurious regression problem during a course at the <a href="https://www.chicagobooth.edu/">Booth school of business</a>. It’s well known among econometricians because it is in the classic text by Hamilton but I don’t think it’s known more widely.</p><p>A first-order measure of association between two variables \(x,y\) is their correlation. Equivalently, we can fit a univariate linear regression to the data:</p><p>\[ y = \alpha + \beta x \]</p><p>If we have \(N\) observations that are independent, given a couple mild assumptions, we get a CLT:<br/></p><p>\[ \sqrt{N}(\hat{\beta}-\beta) \rightarrow N(0,\sigma_{y\mid x}^2/\sigma_x^2), \]</p><p>where \(\sigma_x^2 = \text{var}( x)\) and \(\sigma_{y\mid x}^2 = \text{var}( y-\alpha-\beta x)\).</p><p>We can test for association (\(\beta \not = 0\)) using a standard F-test.</p><p>The independent observation assumption is crucial. Without it, you can get very surprising and unusual behavior.</p><p>Consider observations of pairs \(x_t,y_t\), which are generated from random walks:</p><p>


        </div>
        
          <p class="post-continue">
            <a href="/2016/03/01/spurious-correlation-unit-roots-and-cointegration/">Read on &rarr;</a>
          </p>
        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
          </script>
          <h1 class="post-title">
            
              <a class="post-link" href="/2016/02/11/a-clt-for-the-differential-entropy-of/">A CLT For the Differential entropy of high-dimensional Gaussian Distributions</a>
            
          </h1>

          <p class="post-meta">
            Feb 11, 2016
            
              

 •
  
    
    
      
    
      
        <a href="/tags/central-limit-theorem/">central-limit-theorem</a>
      
    
      
    
      
    
      
    
      
    
      
    
  


            
            
          </p>
        </header>

        <div class="post-content">
          <p>In graduate school most of my research was broadly in the area of multivariate analysis, of which covariance estimation is an important subject. I gave a seminar talk on <a href="http://arxiv.org/pdf/1309.0482.pdf">this</a> paper when I was in grad school. Often results in random matrix theory can be quite complicated, but the proofs for this problem are surprisingly elegant.</p><p>The differential entropy is defined for a density \(p\) as</p><p>\[ H(p) = -\mathbb{E}_p[\log p(X)] . \]</p><p>For a \(D-\)dimensional Gaussian \(N(\mu,\Sigma)\), this is given by the formula</p><p>\[ H(p) = \frac{D}{2}+\frac{D\log (2\pi)}{2} +\frac{\log \mid \Sigma \mid}{2},\]</p><p>where \( \mid \cdot \mid\) denotes the determinant. So for the Gaussian problem, estimating entropy amounts to estimating the log-determinant of the covariance matrix. Note that one representation for the log-determinant is as the sum of the log-eigenvalues:</p><p>\[ \log \mid \hat{\Sigma} \mid = \sum_i \log \lambda_i .\]</p><p>Since the determinant depends on <i>all </i>of the eigenvalues of the random matrix, which are generally dependent, we might expect getting a limiting distribution would be difficult. In fact, we can derive a relatively simple <i>finite-sample</i> expression for the log-det, which naturally leads to a CLT. </p><p>The determinant and log-determinant of a Gaussian matrix appear frequently in multivariate analysis, such as <a href="https://en.wikipedia.org/wiki/Multivariate_analysis_of_variance">multivariate ANOVA</a> for comparing two multivariate samples and <a href="https://en.wikipedia.org/wiki/Quadratic_classifier#Quadratic_discriminant_analysis">quadratic discriminant analysis</a>, which is a generative method for classification.<br/></p><p>The sample covariance for a sample of i.i.d. random Gaussian vectors \(X_1,\ldots,X_N\) is given by</p><p>\[ \hat{\Sigma} = \frac{1}{N} \sum_{i=1}^N (X_i - \bar{X})(X_i - \bar{X})^\top. \]</p><p>Our estimator of the log-determinant of the population covariance is simply the log-determinant of the sample covariance:</p><p>\[\hat{T} = \log \mid \hat{\Sigma} \mid.\]</p><p>The main theorem of the paper derives a central limit theorem for the log-determinant of the sample covariance matrix</p><p><b>Theorem 1</b>: </p><p>Suppose that \(D\leq N\). Then the log-determinant of the sample covariance satisfies as \(N\rightarrow \infty\),</p><p>\[ \frac{\log \mid \hat{\Sigma}\mid - \tau_{N,D} - \log \mid \Sigma \mid}{\sigma_{N,D}} \rightarrow N(0,1), \]</p><p>where \(\tau_{N,D} = \sum_{k=1}^D \left(\psi(\frac{N-k+1}{2}) - \log(\frac{N}{2})\right) \), \(\psi\) is the digamma function, and \(\sigma_{N,D}^2=\sum_{k=1}^D \frac{1}{N-k+1}\).</p><p><b>Comments</b></p><p>The first interesting observation is the bias to the log-det sample covariance. When \(D\) is fixed as \(N\) grows, the bias disappears asymptotically. In particular, if \(D\) is fixed, the bias and standard deviation are given by</p><p>\[\begin{array}{ll} \tau_{N,D} = \frac{D(D+1)}{2N}, &amp;&amp; \sigma_{N,D} = \sqrt{2D/N} \end{array}. \]</p><p><b>Proof (Sketch)</b></p><p>Note that \(\hat{\Sigma}\) has the same distribution as a sum \(\frac{1}{N}\sum_{i} Z_iZ_i^\top\), where \(Z_i\) are i.i.d. \(N(0,\Sigma)\). So we have</p><p>\[\begin{array}{ll} \log \mid \hat{\Sigma}\mid - \log \mid \Sigma \mid &amp;=&amp; \log \mid \Sigma^{-&frac12;}\hat{\Sigma}\Sigma^{-&frac12;}  \mid \\ &amp;=&amp; \log \det \mid \frac{1}{N}\sum_i Y_i Y_i^\top \mid, \\ &amp;=&amp; \log \det \hat{I} \end{array}\]</p><p>where \(Y_i \sim N(0,1)\) and \(\hat{I} = \sum_i Y_i Y_i^\top \). Using the <a href="https://en.wikipedia.org/wiki/Wishart_distribution#Bartlett_decomposition">Bartlett decomposition</a>, we can Cholesky factorize \(\hat{I}\) by</p><p>\[ \hat{I} = AA^\top, \]</p><p>where \(A\) is a lower-triangular matrix with independent random entries. Since the Cholesky factor is triangular, it’s determinant is the product of the diagonals, and \(\mid \hat{I} \mid = \mid A\mid ^2\), so the log-determinant of \(\hat{I}\) will be a particular sum of i.i.d. random variables. In particular, it may be expressed as a particular sum of independent log-chi-square distributions, given by</p><p>\[ \log \det (N \hat{I}) \sim \sum_{i=1}^D \log (\chi_{N-i+1}^2). \]</p><p>Finding the particular forms of the bias and standard deviation of the estimator for the different regimes requires computing the expected value and variance of this random variable, which you can read in Section 5.1.</p><p><b>Theorem 2</b></p><p>In addition to a CLT, the authors derive the (non-asymptotic) risk of this estimator with respect to squared-error loss.</p><p>If \(D \leq N\), the estimator satisfies</p><p>\[ \mathbb{E}[(\hat{T}-\log \mid \Sigma\mid)^2] \leq -2\log(1-D/N)+\frac{10D}{3N(N-D)}. \]</p><p>The paper has some interesting insights on the risk for this problem. If \(D/N\rightarrow 0\), then our estimator asymptotically achieves the <i>minimax risk</i> both in rate and in constant, which is \(2D/N\): it is asymptotically optimal in the minimax sense. If \(D/N\rightarrow r\in (0,1]\), the minimax risk for this problem is non-vanishing, so it is not possible to consistently estimate the log-determinant.</p><p>Finally, a surprising result. We may think that we can improve these results in the high-dimensional setting if we add some constraints on the form of \(\Sigma\), and then design a specialized estimate of \(\Sigma\) for that class. There have been many such considerations in the literature:such as assuming <a href="http://projecteuclid.org/euclid.aos/1231165180">sparsity </a>of \(\Sigma\), or <a href="http://projecteuclid.org/euclid.ejs/1214491853">sparsity </a>of \(\Sigma^{-1}\).</p><p><b>Theorem 5</b></p><p>Suppose \(\Sigma\) belongs to the collection of bounded diagonal matrices. Then the minimax risk is lower bounded by \(c D/N\), where \(0&lt;c\leq 2\).</p><p>Since any class of covariance matrices of interest includes bounded diagonal matrices as a subset, we cannot hope to consistently estimate the log-det if \(D/N \not\rightarrow 0\). This is a disappointing result, because there is a huge literature on high-dimensional estimators for covariance matrices. While the estimates themselves may be consistent by some notion of risk, they cannot be “plugged-in” to estimate the log-det consistently. </p><p>I have been to talks where it has been stated, “often we are primarily interested in the high-dimensional covariance matrix as an input to something else, like a QDA classifier”, but the theory suggests this is misguided.</p>

        </div>
        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
          </script>
          <h1 class="post-title">
            
              <a class="post-link" href="/2016/02/02/pitman-closeness-a-strange-alternative-to-risk/">Pitman Closeness, a strange alternative to risk</a>
            
          </h1>

          <p class="post-meta">
            Feb 2, 2016
            
              

 •
  
    
    
      
    
      
    
      
    
      
    
      
        <a href="/tags/decision-theory/">decision-theory</a>
      
    
      
    
      
    
  


            
            
          </p>
        </header>

        <div class="post-content">
          <p>Here is the curious story of a one-time alternative to the accepted notions of statistical optimality. Today, when we talk about decision theory, we think of the <i>risk</i>, the expected loss of a particular decision rule. However at one point in the history of Statistics, there was another candidate. Pitman Closeness makes a lot of sense conceptually, and generated quite a bit of interest in past decades. However, it can lead you to some strange conclusions. As such, it has not lasted the test of time.</p><p>Statistical decision theory begins by considering an observation \(x\) drawn from a distribution \(F(x\mid \theta)\) parametrized by \(\theta\), a <i>decision rule </i>\(\delta\) which is a measurable function of the data \(x\), and a<i> loss function</i> \(L(\theta,\delta(x))\), which measures the loss from taking some action \(\delta\). The <i>risk </i>is defined as<br/></p><p><i>\[ R(\theta,\delta) = \mathbb{E}_F[ L(\theta,\delta(X))], \]</i></p><p>which measures the expected loss averaging over the distribution \(F\). Decision-theoretic concepts of optimality are defined with respect to the risk. For example, a decision rule \(\delta^*\) is <i>minimax</i> if it minimizes the maximum risk over a class of decision rules \(\mathcal{D}\): for all \(\delta\in\mathcal{D}\),</p><p>\[ \max_\theta R(\theta,\delta^*) \leq \max_\theta R(\theta,\delta). \]</p><p>In general there could be multiple minimax decision rules.</p><p>Another desirable property of a decision rule \(\delta^*\) is <i>admissability</i>, which says that there is no decision rule \(\delta\) which <i>dominates</i> \(\delta_1\). A decision rule \(\delta\) dominates \(\delta_1\) if</p><p>\[ R(\theta,\delta) \leq R(\theta,\delta^*) \]</p><p>for all \(\theta\), with strict inequality for some \(\theta\). Admissability is a desirable but not sufficient measure of optimality. For example a constant estimator is usually admissable (it has the minimal possible risk when the parameter takes its value). An admissable rule need not be minimax, nor a minimax rule admissable. Thus if a decision rule is both admissable and minimax it should be put in high regard.</p><p><b>Pitman Closeness</b><br/></p><p>An interesting alternative to comparing estimators according to risk was proposed in <a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=2028260&amp;fileId=S0305004100019563">Pitman, 1937</a>. A decision rule \(\delta_1\) <i>Pitman dominates </i>\(\delta_2\), denoted \(\delta_1 \overset{P}{\succ}\delta_2\) if for all \(\theta\),</p><p>\[ P(L(\theta,\delta_1) \leq L(\theta,\delta_2)) &gt; &frac12;. \]</p><p>Pitman domination simply says that it is more probable than not that one decision rule has smaller loss than the other. This criterion appears to have some advantages over risk. Firstly, it considers the entire distribution of the loss \(L(\theta,\delta)\), while the risk is just the expectation over the loss. Also crucially, it involves the <i>joint distribution </i>of the pair \(\{L(\theta,\delta_1),L(\theta,\delta_2)\}\). At first glance, this looks like a good way to compare decision rules.</p><!-- more --><p><b>A decision rule which Pitman dominates the normal sample mean</b></p><p>Consider an i.i.d. sample \(X_1,\ldots,X_N\) from a univariate normal distribution, \(Normal(\theta,1)\). It is well known that the sample mean \(\bar{X}\) is unbiased, UMVUE, minimax and admissable (in terms of the squared error loss). The univariate normal means problem is pretty much the most cut-and-dry problem in statistics, and any good decision theory framework should tell you to use the sample mean. Weirdly enough, <i>there is an estimator which Pitman dominates the sample mean!</i>. This example comes from <a href="http://www.sciencedirect.com/science/article/pii/0001870875901140">Efron, 1975</a>.</p><p>Define \(X^*\) by</p><p>\[ X^* = \bar{X} - \Delta(\bar{X}), \]</p><p>where \(\Delta\) is an odd function, which for \(x\geq 0\) takes the values,</p><p>\[ \Delta(x) = \frac{1}{2\sqrt{N}} \min \left\{\sqrt{N}x, \Phi(-\sqrt{N}x)\right\}. \]</p><p>Then \(X^* \overset{P}{\succ} \bar{X}\).</p><p>What’s going on? Observe that \(X^*\) is a function of \(\bar{X}\) which shrinks it towards zero. This sounds unintuitive, since the rule dominates the sample mean for <i>any </i>\(\theta\), even when it is very far from 0! But there is actually a similar phenomenon in decision theory, which is known as the <a href="https://en.wikipedia.org/wiki/Stein%27s_example">Stein effect</a>, in which you may construct an estimator which dominates a minimax estimator by shrinking it towards zero. In a monumental work, Charles Stein proved that this <i>Stein effect</i> occurs when <i>estimating a normal mean</i> <i>vector of length at least 3, </i>when your loss function is the squared-error loss. The Stein estimator shook the Statistics field when it was discovered, and spawned a pet industry of theoretical work relating to shrinkage estimators (and also set the foundations for the use of regularization today, which is less appreciated by most practitioners). But no estimator dominates the sample mean in the univariate scenario. The peculiar thing about Pitman domination is you get a Stein-type effect even for estimation of a single normal mean. Even in the simplest possible estimation problem, we end up with a paradoxical result.</p><p><b>Pitman Closeness is not transitive</b></p><p>The next strange property of Pitman closeness is that it is not transitive. In general, it can’t formulate an ordering over a set of decision rules, and as a consequence there may be no Pitman dominant decision rule. This example comes from <a href="https://scholar.google.com/scholar?cluster=14941327657343594317&amp;hl=en&amp;as_sdt=0,14">Robert, 2007</a>.</p><p>Let \(x \sim Uniform(-0.9\theta,1.1\theta)\). Then consider the decision rules \(\delta_0(x) = x,\delta_1(x)=0.9\mid x \mid\), and \(\delta_2(x) = 3.2 \mid x \mid \). Then \(\delta_0 \overset{P}{\succ}\delta_1,\delta_1 \overset{P}{\succ}\delta_2,\) and \(\delta_2 \overset{P}{\succ}\delta_0\).</p><p><b>Conclusion</b></p><p>Pitman closeness is an interesting idea, and was the focus of a fair amount of research at one time. There was a <a href="https://scholar.google.com/scholar?cluster=2841823111824779602&amp;hl=en&amp;as_sdt=0,14">book</a> devoted to it. But today it is a little-known curiosity. Pitman closeness begins with eminently reasonable foundations but then leads to too many conclusions that go against what we might want from a reasonable decision theory. The risk approach to decision theory, despite some it’s own paradoxes, is the accepted benchmark in statisics.</p>

        </div>
        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
          </script>
          <h1 class="post-title">
            
              <a class="post-link" href="/2016/01/25/the-kernel-density-estimator-minimizes-the/">The kernel density estimator minimizes the regularized Tsallis score</a>
            
          </h1>

          <p class="post-meta">
            Jan 25, 2016
            
              

 •
  
    
    
      
    
      
    
      
        <a href="/tags/data/">data</a>,
      
    
      
    
      
    
      
    
      
    
  
    
    
      
        <a href="/tags/statistics/">statistics</a>,
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  
    
    
      
    
      
    
      
    
      
        <a href="/tags/nonparametric-statistics/">nonparametric-statistics</a>
      
    
      
    
      
    
      
    
  


            
            
          </p>
        </header>

        <div class="post-content">
          <p>The kernel density estimator (KDE) is a simple and popular tool for nonparametric density estimation. In one-dimension it is given by</p><p>\[ \hat{p}_{KDE}(x) = \frac{1}{Nh} \sum_{i=1}^NK\left(\frac{x-X_i}{h}\right). \]</p><p>\(K\) is a kernel (let’s say a variance 1 density for simplicity). It has a simple closed form, and there is extensive literature on theoretical justifications for KDE. One conceptual difficulty with KDE is that it is not represented as a solution to an optimization problem. Most statistics and ML algorithms, from PCA to SVM to k-means are either formulated as an optimization or may be alternatively formulated as the solution to one. For me, this gives me better intuition, and often provides a decision-theoretic justification for the problem. For example, the popular ML algorithm <a href="https://en.wikipedia.org/wiki/AdaBoost#Boosting_as_gradient_descent">AdaBoost</a>, the first boosting algorithm, benefited from new insights and extensions when it was <a href="http://www.jstor.org/stable/2699986">discovered</a> that it is essentially a greedy algorithm for optimizing the exponential classification loss.</p><p>A few years ago I came across the paper <a href="http://ysidro.econ.uiuc.edu/~roger/research/densiles/heat.pdf">What do Kernel Density Estimators Optimize?</a> by Koenker et al. It has some interesting connections between the <a href="https://en.wikipedia.org/wiki/Heat_equation">heat equation</a> and KDEs, but the theorem I find most interesting is the following:</p><p><b>Theorem 1: </b>Let \(E_n[\cdot]\) denote the expectation operator with respect to the empirical distribution of a sample \(X_1,\ldots,X_N\). The solution to</p><p>\[\min_f \left\{ -\mathbb{E}_n[f(x,\lambda)]+ \frac{1}{2} \intop f(x,\lambda)^2 dx +\frac{\lambda}{2} \intop \left(\frac{\partial f(x,\lambda)}{\partial x}\right)^2 dx \right\}\]</p><p>is given by</p><p>\[ \hat{p}(x) = \frac{1}{2\sqrt{\lambda n}} \sum_{i=1}^N \exp \{ -\mid x-X_i \mid /\sqrt{\lambda} \}. \]</p><p>This is precisely the KDE with a <a href="https://en.wikipedia.org/wiki/Laplace_distribution">Laplace kernel</a>. In section 4 of the paper, the authors show that by changing the third term in the optimization to a different penalty, the solution is a KDE with a different choice of kernel. Like with other <a href="https://scholar.google.com/scholar?cluster=10372793773687401003&amp;hl=en&amp;as_sdt=0,14">kernel methods</a> such as<a href="https://en.wikipedia.org/wiki/Support_vector_machine"> kernel SVM</a> , each kernel corresponds to a norm in a <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">reproducing kernel Hilbert space</a>. Changing the penalty term to that particular norm will result in a kernel estimator with the corresponding kernel for that RKHS. For example, we could change the penalty from </p><p>\( \intop \left(\frac{\partial f(x,\lambda)}{\partial x}\right)^2 dx \) to \(


        </div>
        
          <p class="post-continue">
            <a href="/2016/01/25/the-kernel-density-estimator-minimizes-the/">Read on &rarr;</a>
          </p>
        
      </li>
    
  </ul>

  
  <div class="pagination">
    
      <a class="previous" href="/posts/2/">&laquo; Older</a>
    

    
  </div>



</div>
      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy; Eric Janofsky - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="http://localhost:4000/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>

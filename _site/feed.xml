<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-01-22T16:34:01-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Eric Janofsky</title><subtitle></subtitle><entry><title type="html">Uninformative priors say something informative about the posterior predictions</title><link href="http://localhost:4000/2017/09/21/uninformative-priors-say-something-informative.html" rel="alternate" type="text/html" title="Uninformative priors say something informative about the posterior predictions" /><published>2017-09-21T17:52:42-04:00</published><updated>2017-09-21T17:52:42-04:00</updated><id>http://localhost:4000/2017/09/21/uninformative-priors-say-something-informative</id><content type="html" xml:base="http://localhost:4000/2017/09/21/uninformative-priors-say-something-informative.html">&lt;p&gt;I took a class in Bayesian statistics in year two of graduate school. I wouldn't call myself a Bayesian -- as an industry practitioner, I take the &quot;whatever works&quot; approach. It's done me a lot of good to have a broad toolset, pulling out one tool or another based on what I think is best for the business problem at hand. This tends to be the attitude in ML research, but statistics can still be pretty clan-ny with various &quot;Bayesian&quot; societies and affiliations.&lt;/p&gt;


&lt;p&gt;Even within the Bayesian community there are different attitudes on how to do Bayesian statistics. For example, one camp advocates for non-informative priors, which aim to not incorporate any &quot;subjective judgements&quot; on the parameters. Supporters call this &quot;objective Bayes&quot;.&lt;/p&gt;

&lt;p&gt;I remember coming across a puzzling example of the use of non-informative priors which made me doubt whether a prior could really ever be &quot;objective&quot;. I'll describe this example below. Interestingly, it also connects to a well-known probability paradox called the &quot;exchange paradox&quot;, which I discuss later. &lt;/p&gt;

&lt;p&gt;

Suppose we have a set of \(n\) i.i.d. exponential data, so the likelihood takes the form&lt;/p&gt;&lt;p&gt;\[\begin{array}{ll}p(x_1,\ldots,x_n) &amp;amp;=&amp;amp; \lambda^n e^{-\lambda n \bar{x}}. \end{array}\] &lt;br/&gt;&lt;/p&gt;&lt;p&gt;We put an improper prior over \(\lambda\), \(p(\lambda)\propto 1 \). This basically says before we observe the data, we believe the parameter is equally likely to be any positive real number. It’s not a proper prior, but it results in a valid posterior so let’s go with it.&lt;/p&gt;&lt;p&gt;The posterior distribution for \(\lambda\) is easily calculated as&lt;/p&gt;&lt;p&gt;\[\begin{array}{ll}p(\lambda \mid \bar{x}) &amp;amp;\propto&amp;amp; \lambda^n e^{-\lambda n \bar{x}}. \end{array}\]&lt;/p&gt;&lt;p&gt;By inspection one recognizes this as a Gamma distribution with shape parameter \(n+1\) and scale parameter \(\frac{1}{n\bar{x}}\). That means the posterior mean is&lt;/p&gt;&lt;p&gt;\[ \mathbb{E}_{post} [\lambda] = \frac{n+1}{n} \cdot \frac{1}{\bar{x}}. \]&lt;/p&gt;&lt;p&gt;The strange thing about this is that &lt;i&gt;no matter what the sample is&lt;/i&gt;, the posterior &lt;i&gt;always &lt;/i&gt;expects a value larger than the natural sample estimate (\(\frac{1}{\bar{x}}\))!&lt;/p&gt;&lt;p&gt;A more tangible example of this problem is something called the &lt;i&gt;&lt;a href=&quot;http://faculty.chicagobooth.edu/nicholas.polson/teaching/41900/exchange-1.pdf&quot;&gt;exchange paradox&lt;/a&gt;&lt;/i&gt;. You are told to choose between two envelopes filled with money, one has twice as much as the other with unknown amounts; I get the other one. You choose one envelope and it contains $100.&lt;/p&gt;

&lt;p&gt;Next, you reason that the other envelope is equally likely to contain $50 if you’re holding the larger envelope, or $200 if you’re holding the smaller envelope. Thus, you expect to get a payoff of \(0.5 \cdot $50 + 0.5 \cdot $200 = $125\) if you trade with me. I make the same calculation with the amount in my envelope and come to the same conclusion; we &lt;b&gt;both&lt;/b&gt; believe we will likely get a larger payoff by exchanging for the other envelope. How can this be?&lt;/p&gt;&lt;p&gt;There’s been endless discussion on this classic paradox, but I want to think through this problem like a Bayesian. Let’s call the quantity in the larger envelope \(X\) and the quantity you selected \(Y\); \(X\) could either take the value \(Y\) or \(2Y\) depending on whether you chose the larger or smaller amount. Now suppose you have a prior on your expectations of the value of the larger envelope, call it \(p(x)\). An application of Bayes rule gives that&lt;/p&gt;&lt;p&gt;\[ p(X = y \mid Y=y) = \frac{p(x)}{p(x/2)+p(x)}.\]&lt;/p&gt;&lt;p&gt;Similarly, \( p(X = 2y \mid Y=y) = \frac{p(x/2)}{p(x/2)+p(x)}\) since the likelihood of the two values must sum to one. We can now calculate the expected payout from switching:&lt;/p&gt;&lt;p&gt;\[ E = \frac{xp(x/2) }{2p(x)+2p(x/2)} + \frac{2xp(x)}{p(x)+p(x/2)} \]&lt;/p&gt;&lt;p&gt;Now, suppose that \(p(x)\) is the prior we considered above, uniform over the positive reals. The expected value becomes&lt;br/&gt;&lt;/p&gt;&lt;p&gt;\[ E = \frac{x}{4} + x = 1.25 x\]&lt;/p&gt;&lt;p&gt;We come to the same conclusion as the argument I made above, that it is &lt;i&gt;always&lt;/i&gt; better to switch envelopes!&lt;/p&gt;&lt;p&gt;However, other choices of priors will yield less “paradoxical” results. For example the prior \(p(\lambda) \propto \frac{1}{\lambda} \), gives expected payoff from switching envelopes of \(x\), even odds.&lt;/p&gt;&lt;p&gt;My take-away is that non-informative priors still have strong consequences for inference in relation to the observed data, despite having “no information”. If you use the non-informative prior for the exchange game you are saying if your envelope had $20 trillion, you have no inclination that you hold the larger envelope than if it had 20 cents. If the conclusions of the exchange paradox don’t sit right with you, it’s because you don’t really believe in the prior you chose &amp;ndash; maybe the non-informative prior wasn’t what you really wanted!&lt;/p&gt;</content><author><name></name></author><summary type="html">I took a class in Bayesian statistics in year two of graduate school. I wouldn't call myself a Bayesian -- as an industry practitioner, I take the &quot;whatever works&quot; approach. It's done me a lot of good to have a broad toolset, pulling out one tool or another based on what I think is best for the business problem at hand. This tends to be the attitude in ML research, but statistics can still be pretty clan-ny with various &quot;Bayesian&quot; societies and affiliations.</summary></entry><entry><title type="html">Spurious correlation, unit roots and cointegration</title><link href="http://localhost:4000/2016/03/01/spurious-correlation-unit-roots-and-cointegration.html" rel="alternate" type="text/html" title="Spurious correlation, unit roots and cointegration" /><published>2016-03-01T10:01:51-05:00</published><updated>2016-03-01T10:01:51-05:00</updated><id>http://localhost:4000/2016/03/01/spurious-correlation-unit-roots-and-cointegration</id><content type="html" xml:base="http://localhost:4000/2016/03/01/spurious-correlation-unit-roots-and-cointegration.html">&lt;p&gt;I learned about the spurious regression problem during a course at the &lt;a href=&quot;https://www.chicagobooth.edu/&quot;&gt;Booth school of business&lt;/a&gt;. It’s well known among econometricians because it is in the classic text by Hamilton but I don’t think it’s known more widely.&lt;/p&gt;&lt;p&gt;A first-order measure of association between two variables \(x,y\) is their correlation. Equivalently, we can fit a univariate linear regression to the data:&lt;/p&gt;&lt;p&gt;\[ y = \alpha + \beta x \]&lt;/p&gt;&lt;p&gt;If we have \(N\) observations that are independent, given a couple mild assumptions, we get a CLT:&lt;br/&gt;&lt;/p&gt;&lt;p&gt;\[ \sqrt{N}(\hat{\beta}-\beta) \rightarrow N(0,\sigma_{y\mid x}^2/\sigma_x^2), \]&lt;/p&gt;&lt;p&gt;where \(\sigma_x^2 = \text{var}( x)\) and \(\sigma_{y\mid x}^2 = \text{var}( y-\alpha-\beta x)\).&lt;/p&gt;&lt;p&gt;We can test for association (\(\beta \not = 0\)) using a standard F-test.&lt;/p&gt;&lt;p&gt;The independent observation assumption is crucial. Without it, you can get very surprising and unusual behavior.&lt;/p&gt;&lt;p&gt;Consider observations of pairs \(x_t,y_t\), which are generated from random walks:&lt;/p&gt;&lt;p&gt;

\[\begin{array}{ll}x_t &amp;amp;=&amp;amp; x_{t-1} + u_t, \\ y_t &amp;amp;=&amp;amp; y_{t-1} + w_t,  \end{array}\]

&lt;br/&gt;&lt;/p&gt;&lt;p&gt;where the errors \(u_t,w_t \sim\ N(0,1)\), are independent and \(x_0 = y_0 =0\), so both series are independent. Econometricians like to call processes like this “unit root” processes.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Theorem (&lt;a href=&quot;http://press.princeton.edu/titles/5386.html&quot;&gt;Hamilton, 1994&lt;/a&gt;)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Suppose we have samples \(\{(x_0,y_0),\ldots,(x_T,y_T)\}\) generated as described above. Then&lt;/p&gt;&lt;p&gt;\[ \hat{\beta} \rightarrow \frac{ \intop_{0}^1 W_1 ( r) W_2( r) dr}{\intop_{0}^{1} W_2( r)^2 dr}, \]&lt;/p&gt;&lt;p&gt;where \(W_1,W_2\) are independent &lt;a href=&quot;https://en.wikipedia.org/wiki/Brownian_motion&quot;&gt;Brownian motions&lt;/a&gt; [1].&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Since the two series are independent, we expect the unscaled sample correlation to converge to zero, \(\hat{\beta}\rightarrow 0\). But the theorem shows it actually converges to a &lt;i&gt;random &lt;/i&gt;quantity. Since the limiting distribution doesn’t have a closed-form, I plot a simulation of it below. It is not that unusual for the regression coefficient to converge to a number larger than 1 in absolute value.&lt;/p&gt;&lt;figure data-orig-width=&quot;552&quot; data-orig-height=&quot;401&quot; class=&quot;tmblr-full&quot;&gt;&lt;img src=&quot;https://66.media.tumblr.com/435b215b94442202dd6fd2b001b9aaff/tumblr_inline_o34v18AZon1tlyjch_540.png&quot; alt=&quot;image&quot; data-orig-width=&quot;552&quot; data-orig-height=&quot;401&quot;/&gt;&lt;/figure&gt;&lt;p&gt;Figure 2: Simulation of regression coefficient distribution&lt;/p&gt;&lt;p&gt;What does this mean? As the number of samples increases, the sample correlation will actually approach something nonzero (with probability one). And so in the large-\(n\) limit, the F-test will always reject the hypothesis of association. 

As a consequence we can easily generate a simulation of two independent variables which the F-test says are &lt;i&gt;certainly&lt;/i&gt; dependent!&lt;/p&gt;&lt;figure data-orig-width=&quot;856&quot; data-orig-height=&quot;780&quot; class=&quot;tmblr-full&quot;&gt;&lt;img src=&quot;https://66.media.tumblr.com/e7e5b1242c938e8aae92f00b068b92af/tumblr_inline_o34etoj7a31tlyjch_540.png&quot; alt=&quot;image&quot; data-orig-width=&quot;856&quot; data-orig-height=&quot;780&quot;/&gt;&lt;/figure&gt;&lt;p&gt;Figure 2: Two independent random walks. n=100, \(R^2\)=0.69, F-test p-value &amp;lt; 2.2e-16.&lt;/p&gt;&lt;!-- more --&gt;&lt;p&gt;&lt;b&gt;Cointegration&lt;/b&gt;&lt;/p&gt;&lt;p&gt;So what to do? It’s clear that an F-test is &lt;i&gt;certainly &lt;/i&gt;the wrong thing to do to test association between two nonstationary time series, since the specificity of the test is asymptotically zero. A better approach is to test for cointegration.&lt;/p&gt;&lt;p&gt;Two processes are &lt;i&gt;cointegrated &lt;/i&gt;if they are each marginally nonstationary (unit root) processes, and there exists a constant \(\gamma\) such that&lt;/p&gt;&lt;p&gt;\[ e_t = y_t -\gamma x_t \]&lt;/p&gt;&lt;p&gt;is stationary.&lt;/p&gt;&lt;p&gt;A classic example are bid/ask prices. The bid and ask are respectively the posted price for immediate sale or purchase by a market maker.  These prices generally differ&amp;ndash; their difference is called the &lt;i&gt;spread&lt;/i&gt;. But if they differ by too large an amount, someone else will inevitably enter the market to provide liquidity, and the spread will revert back towards zero. Thus the spread&lt;/p&gt;&lt;p&gt;\[ s_t = p_t^a - p_t ^b \]&lt;/p&gt;&lt;p&gt;should be a stationary process.&lt;/p&gt;&lt;figure data-orig-width=&quot;663&quot; data-orig-height=&quot;551&quot; class=&quot;tmblr-full&quot;&gt;&lt;img src=&quot;https://66.media.tumblr.com/67bfd7663b709f47b78c7a8ce68799cb/tumblr_inline_o37mscGPQ51tlyjch_540.png&quot; alt=&quot;image&quot; data-orig-width=&quot;663&quot; data-orig-height=&quot;551&quot;/&gt;&lt;/figure&gt;&lt;p&gt;Figure 3: Bid/ask prices and the bid/ask spread for a stock&lt;/p&gt;&lt;p&gt;To test for coinegration, there are two possibilities:&lt;/p&gt;&lt;p&gt;1. If the constant \(\gamma\) is known (perhaps in the bid/ask scenario we are willing to assume \(\gamma \)= 1), compute the residual series \(e_t\) and perform the &lt;a href=&quot;https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test&quot;&gt;Dickey-Fuller test&lt;/a&gt; for a unit root.&lt;/p&gt;&lt;p&gt;2. If the constant \(\gamma\) is unknown, estimate \(\hat{\gamma}\) with regression, and perform a special “augmented Dickey-Fuller test”.&lt;/p&gt;&lt;figure data-orig-width=&quot;2080&quot; data-orig-height=&quot;820&quot; class=&quot;tmblr-full&quot;&gt;&lt;img src=&quot;https://66.media.tumblr.com/e9989cb1b3f9cef40686cec064955790/tumblr_inline_o3653vvOYM1tlyjch_540.png&quot; alt=&quot;image&quot; data-orig-width=&quot;2080&quot; data-orig-height=&quot;820&quot;/&gt;&lt;/figure&gt;&lt;p&gt;Figure 4: A spurious correlation&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;[1] I won’t get into what a Brownian motion is, or what the integral over a Brownian motion is. You can look at the original derivation in the source.&lt;/p&gt;</content><author><name></name></author><summary type="html">I learned about the spurious regression problem during a course at the Booth school of business. It’s well known among econometricians because it is in the classic text by Hamilton but I don’t think it’s known more widely.A first-order measure of association between two variables \(x,y\) is their correlation. Equivalently, we can fit a univariate linear regression to the data:\[ y = \alpha + \beta x \]If we have \(N\) observations that are independent, given a couple mild assumptions, we get a CLT:\[ \sqrt{N}(\hat{\beta}-\beta) \rightarrow N(0,\sigma_{y\mid x}^2/\sigma_x^2), \]where \(\sigma_x^2 = \text{var}( x)\) and \(\sigma_{y\mid x}^2 = \text{var}( y-\alpha-\beta x)\).We can test for association (\(\beta \not = 0\)) using a standard F-test.The independent observation assumption is crucial. Without it, you can get very surprising and unusual behavior.Consider observations of pairs \(x_t,y_t\), which are generated from random walks:</summary></entry><entry><title type="html">A CLT For the Differential entropy of high-dimensional Gaussian Distributions</title><link href="http://localhost:4000/2016/02/11/a-clt-for-the-differential-entropy-of.html" rel="alternate" type="text/html" title="A CLT For the Differential entropy of high-dimensional Gaussian Distributions" /><published>2016-02-11T10:01:40-05:00</published><updated>2016-02-11T10:01:40-05:00</updated><id>http://localhost:4000/2016/02/11/a-clt-for-the-differential-entropy-of</id><content type="html" xml:base="http://localhost:4000/2016/02/11/a-clt-for-the-differential-entropy-of.html">&lt;p&gt;In graduate school most of my research was broadly in the area of multivariate analysis, of which covariance estimation is an important subject. I gave a seminar talk on &lt;a href=&quot;http://arxiv.org/pdf/1309.0482.pdf&quot;&gt;this&lt;/a&gt; paper when I was in grad school. Often results in random matrix theory can be quite complicated, but the proofs for this problem are surprisingly elegant.&lt;/p&gt;&lt;p&gt;The differential entropy is defined for a density \(p\) as&lt;/p&gt;&lt;p&gt;\[ H(p) = -\mathbb{E}_p[\log p(X)] . \]&lt;/p&gt;&lt;p&gt;For a \(D-\)dimensional Gaussian \(N(\mu,\Sigma)\), this is given by the formula&lt;/p&gt;&lt;p&gt;\[ H(p) = \frac{D}{2}+\frac{D\log (2\pi)}{2} +\frac{\log \mid \Sigma \mid}{2},\]&lt;/p&gt;&lt;p&gt;where \( \mid \cdot \mid\) denotes the determinant. So for the Gaussian problem, estimating entropy amounts to estimating the log-determinant of the covariance matrix. Note that one representation for the log-determinant is as the sum of the log-eigenvalues:&lt;/p&gt;&lt;p&gt;\[ \log \mid \hat{\Sigma} \mid = \sum_i \log \lambda_i .\]&lt;/p&gt;&lt;p&gt;Since the determinant depends on &lt;i&gt;all &lt;/i&gt;of the eigenvalues of the random matrix, which are generally dependent, we might expect getting a limiting distribution would be difficult. In fact, we can derive a relatively simple &lt;i&gt;finite-sample&lt;/i&gt; expression for the log-det, which naturally leads to a CLT. &lt;/p&gt;&lt;p&gt;The determinant and log-determinant of a Gaussian matrix appear frequently in multivariate analysis, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_analysis_of_variance&quot;&gt;multivariate ANOVA&lt;/a&gt; for comparing two multivariate samples and &lt;a href=&quot;https://en.wikipedia.org/wiki/Quadratic_classifier#Quadratic_discriminant_analysis&quot;&gt;quadratic discriminant analysis&lt;/a&gt;, which is a generative method for classification.&lt;br/&gt;&lt;/p&gt;&lt;p&gt;The sample covariance for a sample of i.i.d. random Gaussian vectors \(X_1,\ldots,X_N\) is given by&lt;/p&gt;&lt;p&gt;\[ \hat{\Sigma} = \frac{1}{N} \sum_{i=1}^N (X_i - \bar{X})(X_i - \bar{X})^\top. \]&lt;/p&gt;&lt;p&gt;Our estimator of the log-determinant of the population covariance is simply the log-determinant of the sample covariance:&lt;/p&gt;&lt;p&gt;\[\hat{T} = \log \mid \hat{\Sigma} \mid.\]&lt;/p&gt;&lt;p&gt;The main theorem of the paper derives a central limit theorem for the log-determinant of the sample covariance matrix&lt;/p&gt;&lt;p&gt;&lt;b&gt;Theorem 1&lt;/b&gt;: &lt;/p&gt;&lt;p&gt;Suppose that \(D\leq N\). Then the log-determinant of the sample covariance satisfies as \(N\rightarrow \infty\),&lt;/p&gt;&lt;p&gt;\[ \frac{\log \mid \hat{\Sigma}\mid - \tau_{N,D} - \log \mid \Sigma \mid}{\sigma_{N,D}} \rightarrow N(0,1), \]&lt;/p&gt;&lt;p&gt;where \(\tau_{N,D} = \sum_{k=1}^D \left(\psi(\frac{N-k+1}{2}) - \log(\frac{N}{2})\right) \), \(\psi\) is the digamma function, and \(\sigma_{N,D}^2=\sum_{k=1}^D \frac{1}{N-k+1}\).&lt;/p&gt;&lt;p&gt;&lt;b&gt;Comments&lt;/b&gt;&lt;/p&gt;&lt;p&gt;The first interesting observation is the bias to the log-det sample covariance. When \(D\) is fixed as \(N\) grows, the bias disappears asymptotically. In particular, if \(D\) is fixed, the bias and standard deviation are given by&lt;/p&gt;&lt;p&gt;\[\begin{array}{ll} \tau_{N,D} = \frac{D(D+1)}{2N}, &amp;amp;&amp;amp; \sigma_{N,D} = \sqrt{2D/N} \end{array}. \]&lt;/p&gt;&lt;p&gt;&lt;b&gt;Proof (Sketch)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Note that \(\hat{\Sigma}\) has the same distribution as a sum \(\frac{1}{N}\sum_{i} Z_iZ_i^\top\), where \(Z_i\) are i.i.d. \(N(0,\Sigma)\). So we have&lt;/p&gt;&lt;p&gt;\[\begin{array}{ll} \log \mid \hat{\Sigma}\mid - \log \mid \Sigma \mid &amp;amp;=&amp;amp; \log \mid \Sigma^{-&amp;frac12;}\hat{\Sigma}\Sigma^{-&amp;frac12;}  \mid \\ &amp;amp;=&amp;amp; \log \det \mid \frac{1}{N}\sum_i Y_i Y_i^\top \mid, \\ &amp;amp;=&amp;amp; \log \det \hat{I} \end{array}\]&lt;/p&gt;&lt;p&gt;where \(Y_i \sim N(0,1)\) and \(\hat{I} = \sum_i Y_i Y_i^\top \). Using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Wishart_distribution#Bartlett_decomposition&quot;&gt;Bartlett decomposition&lt;/a&gt;, we can Cholesky factorize \(\hat{I}\) by&lt;/p&gt;&lt;p&gt;\[ \hat{I} = AA^\top, \]&lt;/p&gt;&lt;p&gt;where \(A\) is a lower-triangular matrix with independent random entries. Since the Cholesky factor is triangular, it’s determinant is the product of the diagonals, and \(\mid \hat{I} \mid = \mid A\mid ^2\), so the log-determinant of \(\hat{I}\) will be a particular sum of i.i.d. random variables. In particular, it may be expressed as a particular sum of independent log-chi-square distributions, given by&lt;/p&gt;&lt;p&gt;\[ \log \det (N \hat{I}) \sim \sum_{i=1}^D \log (\chi_{N-i+1}^2). \]&lt;/p&gt;&lt;p&gt;Finding the particular forms of the bias and standard deviation of the estimator for the different regimes requires computing the expected value and variance of this random variable, which you can read in Section 5.1.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Theorem 2&lt;/b&gt;&lt;/p&gt;&lt;p&gt;In addition to a CLT, the authors derive the (non-asymptotic) risk of this estimator with respect to squared-error loss.&lt;/p&gt;&lt;p&gt;If \(D \leq N\), the estimator satisfies&lt;/p&gt;&lt;p&gt;\[ \mathbb{E}[(\hat{T}-\log \mid \Sigma\mid)^2] \leq -2\log(1-D/N)+\frac{10D}{3N(N-D)}. \]&lt;/p&gt;&lt;p&gt;The paper has some interesting insights on the risk for this problem. If \(D/N\rightarrow 0\), then our estimator asymptotically achieves the &lt;i&gt;minimax risk&lt;/i&gt; both in rate and in constant, which is \(2D/N\): it is asymptotically optimal in the minimax sense. If \(D/N\rightarrow r\in (0,1]\), the minimax risk for this problem is non-vanishing, so it is not possible to consistently estimate the log-determinant.&lt;/p&gt;&lt;p&gt;Finally, a surprising result. We may think that we can improve these results in the high-dimensional setting if we add some constraints on the form of \(\Sigma\), and then design a specialized estimate of \(\Sigma\) for that class. There have been many such considerations in the literature:such as assuming &lt;a href=&quot;http://projecteuclid.org/euclid.aos/1231165180&quot;&gt;sparsity &lt;/a&gt;of \(\Sigma\), or &lt;a href=&quot;http://projecteuclid.org/euclid.ejs/1214491853&quot;&gt;sparsity &lt;/a&gt;of \(\Sigma^{-1}\).&lt;/p&gt;&lt;p&gt;&lt;b&gt;Theorem 5&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Suppose \(\Sigma\) belongs to the collection of bounded diagonal matrices. Then the minimax risk is lower bounded by \(c D/N\), where \(0&amp;lt;c\leq 2\).&lt;/p&gt;&lt;p&gt;Since any class of covariance matrices of interest includes bounded diagonal matrices as a subset, we cannot hope to consistently estimate the log-det if \(D/N \not\rightarrow 0\). This is a disappointing result, because there is a huge literature on high-dimensional estimators for covariance matrices. While the estimates themselves may be consistent by some notion of risk, they cannot be “plugged-in” to estimate the log-det consistently. &lt;/p&gt;&lt;p&gt;I have been to talks where it has been stated, “often we are primarily interested in the high-dimensional covariance matrix as an input to something else, like a QDA classifier”, but the theory suggests this is misguided.&lt;/p&gt;</content><author><name></name></author><summary type="html">In graduate school most of my research was broadly in the area of multivariate analysis, of which covariance estimation is an important subject. I gave a seminar talk on this paper when I was in grad school. Often results in random matrix theory can be quite complicated, but the proofs for this problem are surprisingly elegant.The differential entropy is defined for a density \(p\) as\[ H(p) = -\mathbb{E}_p[\log p(X)] . \]For a \(D-\)dimensional Gaussian \(N(\mu,\Sigma)\), this is given by the formula\[ H(p) = \frac{D}{2}+\frac{D\log (2\pi)}{2} +\frac{\log \mid \Sigma \mid}{2},\]where \( \mid \cdot \mid\) denotes the determinant. So for the Gaussian problem, estimating entropy amounts to estimating the log-determinant of the covariance matrix. Note that one representation for the log-determinant is as the sum of the log-eigenvalues:\[ \log \mid \hat{\Sigma} \mid = \sum_i \log \lambda_i .\]Since the determinant depends on all of the eigenvalues of the random matrix, which are generally dependent, we might expect getting a limiting distribution would be difficult. In fact, we can derive a relatively simple finite-sample expression for the log-det, which naturally leads to a CLT. The determinant and log-determinant of a Gaussian matrix appear frequently in multivariate analysis, such as multivariate ANOVA for comparing two multivariate samples and quadratic discriminant analysis, which is a generative method for classification.The sample covariance for a sample of i.i.d. random Gaussian vectors \(X_1,\ldots,X_N\) is given by\[ \hat{\Sigma} = \frac{1}{N} \sum_{i=1}^N (X_i - \bar{X})(X_i - \bar{X})^\top. \]Our estimator of the log-determinant of the population covariance is simply the log-determinant of the sample covariance:\[\hat{T} = \log \mid \hat{\Sigma} \mid.\]The main theorem of the paper derives a central limit theorem for the log-determinant of the sample covariance matrixTheorem 1: Suppose that \(D\leq N\). Then the log-determinant of the sample covariance satisfies as \(N\rightarrow \infty\),\[ \frac{\log \mid \hat{\Sigma}\mid - \tau_{N,D} - \log \mid \Sigma \mid}{\sigma_{N,D}} \rightarrow N(0,1), \]where \(\tau_{N,D} = \sum_{k=1}^D \left(\psi(\frac{N-k+1}{2}) - \log(\frac{N}{2})\right) \), \(\psi\) is the digamma function, and \(\sigma_{N,D}^2=\sum_{k=1}^D \frac{1}{N-k+1}\).CommentsThe first interesting observation is the bias to the log-det sample covariance. When \(D\) is fixed as \(N\) grows, the bias disappears asymptotically. In particular, if \(D\) is fixed, the bias and standard deviation are given by\[\begin{array}{ll} \tau_{N,D} = \frac{D(D+1)}{2N}, &amp;amp;&amp;amp; \sigma_{N,D} = \sqrt{2D/N} \end{array}. \]Proof (Sketch)Note that \(\hat{\Sigma}\) has the same distribution as a sum \(\frac{1}{N}\sum_{i} Z_iZ_i^\top\), where \(Z_i\) are i.i.d. \(N(0,\Sigma)\). So we have\[\begin{array}{ll} \log \mid \hat{\Sigma}\mid - \log \mid \Sigma \mid &amp;amp;=&amp;amp; \log \mid \Sigma^{-&amp;frac12;}\hat{\Sigma}\Sigma^{-&amp;frac12;}  \mid \\ &amp;amp;=&amp;amp; \log \det \mid \frac{1}{N}\sum_i Y_i Y_i^\top \mid, \\ &amp;amp;=&amp;amp; \log \det \hat{I} \end{array}\]where \(Y_i \sim N(0,1)\) and \(\hat{I} = \sum_i Y_i Y_i^\top \). Using the Bartlett decomposition, we can Cholesky factorize \(\hat{I}\) by\[ \hat{I} = AA^\top, \]where \(A\) is a lower-triangular matrix with independent random entries. Since the Cholesky factor is triangular, it’s determinant is the product of the diagonals, and \(\mid \hat{I} \mid = \mid A\mid ^2\), so the log-determinant of \(\hat{I}\) will be a particular sum of i.i.d. random variables. In particular, it may be expressed as a particular sum of independent log-chi-square distributions, given by\[ \log \det (N \hat{I}) \sim \sum_{i=1}^D \log (\chi_{N-i+1}^2). \]Finding the particular forms of the bias and standard deviation of the estimator for the different regimes requires computing the expected value and variance of this random variable, which you can read in Section 5.1.Theorem 2In addition to a CLT, the authors derive the (non-asymptotic) risk of this estimator with respect to squared-error loss.If \(D \leq N\), the estimator satisfies\[ \mathbb{E}[(\hat{T}-\log \mid \Sigma\mid)^2] \leq -2\log(1-D/N)+\frac{10D}{3N(N-D)}. \]The paper has some interesting insights on the risk for this problem. If \(D/N\rightarrow 0\), then our estimator asymptotically achieves the minimax risk both in rate and in constant, which is \(2D/N\): it is asymptotically optimal in the minimax sense. If \(D/N\rightarrow r\in (0,1]\), the minimax risk for this problem is non-vanishing, so it is not possible to consistently estimate the log-determinant.Finally, a surprising result. We may think that we can improve these results in the high-dimensional setting if we add some constraints on the form of \(\Sigma\), and then design a specialized estimate of \(\Sigma\) for that class. There have been many such considerations in the literature:such as assuming sparsity of \(\Sigma\), or sparsity of \(\Sigma^{-1}\).Theorem 5Suppose \(\Sigma\) belongs to the collection of bounded diagonal matrices. Then the minimax risk is lower bounded by \(c D/N\), where \(0&amp;lt;c\leq 2\).Since any class of covariance matrices of interest includes bounded diagonal matrices as a subset, we cannot hope to consistently estimate the log-det if \(D/N \not\rightarrow 0\). This is a disappointing result, because there is a huge literature on high-dimensional estimators for covariance matrices. While the estimates themselves may be consistent by some notion of risk, they cannot be “plugged-in” to estimate the log-det consistently. I have been to talks where it has been stated, “often we are primarily interested in the high-dimensional covariance matrix as an input to something else, like a QDA classifier”, but the theory suggests this is misguided.</summary></entry><entry><title type="html">Pitman Closeness, a strange alternative to risk</title><link href="http://localhost:4000/2016/02/02/pitman-closeness-a-strange-alternative-to-risk.html" rel="alternate" type="text/html" title="Pitman Closeness, a strange alternative to risk" /><published>2016-02-02T10:01:58-05:00</published><updated>2016-02-02T10:01:58-05:00</updated><id>http://localhost:4000/2016/02/02/pitman-closeness-a-strange-alternative-to-risk</id><content type="html" xml:base="http://localhost:4000/2016/02/02/pitman-closeness-a-strange-alternative-to-risk.html">&lt;p&gt;Here is the curious story of a proposed alternative to the accepted notions of statistical optimality. Pitman Closeness makes a lot of sense conceptually, and generated quite a bit of interest in past decades. However, it can lead you to some bizarre conclusions.&lt;/p&gt;&lt;p&gt;Statistical decision theory begins by considering an observation \(x\) drawn from a distribution \(F(x\mid \theta)\) parametrized by \(\theta\), a &lt;i&gt;decision rule &lt;/i&gt;\(\delta\) which is a measurable function of the data \(x\), and a&lt;i&gt; loss function&lt;/i&gt; \(L(\theta,\delta(x))\), which measures the loss from taking some action \(\delta\). The &lt;i&gt;risk &lt;/i&gt;is defined as&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;\[ R(\theta,\delta) = \mathbb{E}_F[ L(\theta,\delta(X))], \]&lt;/i&gt;&lt;/p&gt;&lt;p&gt;which measures the expected loss averaging over the distribution \(F\). Decision-theoretic concepts of optimality are defined with respect to the risk. For example, a decision rule \(\delta^*\) is &lt;i&gt;minimax&lt;/i&gt; if it minimizes the maximum risk over a class of decision rules \(\mathcal{D}\): for all \(\delta\in\mathcal{D}\),&lt;/p&gt;&lt;p&gt;\[ \max_\theta R(\theta,\delta^*) \leq \max_\theta R(\theta,\delta). \]&lt;/p&gt;&lt;p&gt;In general there could be multiple minimax decision rules.&lt;/p&gt;&lt;p&gt;Another desirable property of a decision rule \(\delta^*\) is &lt;i&gt;admissability&lt;/i&gt;, which says that there is no decision rule \(\delta\) which &lt;i&gt;dominates&lt;/i&gt; \(\delta_1\). A decision rule \(\delta\) dominates \(\delta_1\) if&lt;/p&gt;&lt;p&gt;\[ R(\theta,\delta) \leq R(\theta,\delta^*) \]&lt;/p&gt;&lt;p&gt;for all \(\theta\), with strict inequality for some \(\theta\). Admissability is a desirable but not sufficient measure of optimality. For example a constant estimator is usually admissable (it has the minimal possible risk when the parameter takes its value). An admissable rule need not be minimax, nor a minimax rule admissable. Thus if a decision rule is both admissable and minimax it should be put in high regard.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Pitman Closeness&lt;/b&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;An interesting alternative to comparing estimators according to risk was proposed in &lt;a href=&quot;http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;amp;aid=2028260&amp;amp;fileId=S0305004100019563&quot;&gt;Pitman, 1937&lt;/a&gt;. A decision rule \(\delta_1\) &lt;i&gt;Pitman dominates &lt;/i&gt;\(\delta_2\), denoted \(\delta_1 \overset{P}{\succ}\delta_2\) if for all \(\theta\),&lt;/p&gt;&lt;p&gt;\[ P(L(\theta,\delta_1) \leq L(\theta,\delta_2)) &amp;gt; &amp;frac12;. \]&lt;/p&gt;&lt;p&gt;Pitman domination simply says that it is more probable than not that one decision rule has smaller loss than the other. This criterion appears to have some advantages over risk. Firstly, it considers the entire distribution of the loss \(L(\theta,\delta)\), while the risk is just the expectation over the loss. Also crucially, it involves the &lt;i&gt;joint distribution &lt;/i&gt;of the pair \(\{L(\theta,\delta_1),L(\theta,\delta_2)\}\). At first glance, this looks like a good way to compare decision rules.&lt;/p&gt;&lt;!-- more --&gt;&lt;p&gt;&lt;b&gt;A decision rule which Pitman dominates the normal sample mean&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Consider an i.i.d. sample \(X_1,\ldots,X_N\) from a univariate normal distribution, \(Normal(\theta,1)\). It is well known that the sample mean \(\bar{X}\) is unbiased, UMVUE, minimax and admissable. Weirdly enough, there is an estimator which Pitman dominates the sample mean. This example comes from &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/0001870875901140&quot;&gt;Efron, 1975&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Define \(X^*\) by&lt;/p&gt;&lt;p&gt;\[ X^* = \bar{X} - \Delta(\bar{X}), \]&lt;/p&gt;&lt;p&gt;where \(\Delta\) is an odd function, which for \(x\geq 0\) takes the values,&lt;/p&gt;&lt;p&gt;\[ \Delta(x) = \frac{1}{2\sqrt{N}} \min \left\{\sqrt{N}x, \Phi(-\sqrt{N}x)\right\}. \]&lt;/p&gt;&lt;p&gt;Then \(X^* \overset{P}{\succ} \bar{X}\).&lt;/p&gt;&lt;p&gt;What’s going on? Observe that \(X^*\) is a function of \(\bar{X}\) which shrinks it towards zero. This sounds unintuitive, since the rule dominates the sample mean for &lt;i&gt;any &lt;/i&gt;\(\theta\), even when it is very far from 0! But there is actually a similar phenomenon in decision theory, which is known as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Stein%27s_example&quot;&gt;Stein effect&lt;/a&gt;, in which you may construct an estimator which dominates a minimax estimator by shrinking it towards zero. In a monumental work, Charles Stein proved that this &lt;i&gt;Stein effect&lt;/i&gt; occurs when &lt;i&gt;estimating a normal mean&lt;/i&gt; &lt;i&gt;vector of length at least 3, &lt;/i&gt;when your loss function is the squared-error loss. But no estimator dominates the sample mean in the univariate scenario. The peculiar thing about Pitman domination is you get a Stein-type effect even for estimation of a single normal mean. Even in the simplest possible estimation problem, we end up with a paradoxical result.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Pitman Closeness is not transitive&lt;/b&gt;&lt;/p&gt;&lt;p&gt;The next strange property of Pitman closeness is that it is not transitive. In general, it can’t formulate an ordering over a set of decision rules, and as a consequence there may be no Pitman dominant decision rule. This example comes from &lt;a href=&quot;https://scholar.google.com/scholar?cluster=14941327657343594317&amp;amp;hl=en&amp;amp;as_sdt=0,14&quot;&gt;Robert, 2007&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Let \(x \sim Uniform(-0.9\theta,1.1\theta)\). Then consider the decision rules \(\delta_0(x) = x,\delta_1(x)=0.9\mid x \mid\), and \(\delta_2(x) = 3.2 \mid x \mid \). Then \(\delta_0 \overset{P}{\succ}\delta_1,d_1 \overset{P}{\succ}\delta_2,\) and \(d_2 \overset{P}{\succ}\delta_0\).&lt;/p&gt;&lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Pitman closeness is an interesting idea, and was the focus of a fair amount of research at one time. There was a &lt;a href=&quot;https://scholar.google.com/scholar?cluster=2841823111824779602&amp;amp;hl=en&amp;amp;as_sdt=0,14&quot;&gt;book&lt;/a&gt; devoted to it. But today it is a little-known curiosity. Pitman closeness begins with eminently reasonable foundations but then leads to too many conclusions that go against what we might want from a reasonable decision theory. The risk approach to decision theory, despite some it’s own paradoxes, is the accepted benchmark in statisics.&lt;/p&gt;</content><author><name></name></author><summary type="html">Here is the curious story of a proposed alternative to the accepted notions of statistical optimality. Pitman Closeness makes a lot of sense conceptually, and generated quite a bit of interest in past decades. However, it can lead you to some bizarre conclusions.Statistical decision theory begins by considering an observation \(x\) drawn from a distribution \(F(x\mid \theta)\) parametrized by \(\theta\), a decision rule \(\delta\) which is a measurable function of the data \(x\), and a loss function \(L(\theta,\delta(x))\), which measures the loss from taking some action \(\delta\). The risk is defined as\[ R(\theta,\delta) = \mathbb{E}_F[ L(\theta,\delta(X))], \]which measures the expected loss averaging over the distribution \(F\). Decision-theoretic concepts of optimality are defined with respect to the risk. For example, a decision rule \(\delta^*\) is minimax if it minimizes the maximum risk over a class of decision rules \(\mathcal{D}\): for all \(\delta\in\mathcal{D}\),\[ \max_\theta R(\theta,\delta^*) \leq \max_\theta R(\theta,\delta). \]In general there could be multiple minimax decision rules.Another desirable property of a decision rule \(\delta^*\) is admissability, which says that there is no decision rule \(\delta\) which dominates \(\delta_1\). A decision rule \(\delta\) dominates \(\delta_1\) if\[ R(\theta,\delta) \leq R(\theta,\delta^*) \]for all \(\theta\), with strict inequality for some \(\theta\). Admissability is a desirable but not sufficient measure of optimality. For example a constant estimator is usually admissable (it has the minimal possible risk when the parameter takes its value). An admissable rule need not be minimax, nor a minimax rule admissable. Thus if a decision rule is both admissable and minimax it should be put in high regard.Pitman ClosenessAn interesting alternative to comparing estimators according to risk was proposed in Pitman, 1937. A decision rule \(\delta_1\) Pitman dominates \(\delta_2\), denoted \(\delta_1 \overset{P}{\succ}\delta_2\) if for all \(\theta\),\[ P(L(\theta,\delta_1) \leq L(\theta,\delta_2)) &amp;gt; &amp;frac12;. \]Pitman domination simply says that it is more probable than not that one decision rule has smaller loss than the other. This criterion appears to have some advantages over risk. Firstly, it considers the entire distribution of the loss \(L(\theta,\delta)\), while the risk is just the expectation over the loss. Also crucially, it involves the joint distribution of the pair \(\{L(\theta,\delta_1),L(\theta,\delta_2)\}\). At first glance, this looks like a good way to compare decision rules.A decision rule which Pitman dominates the normal sample meanConsider an i.i.d. sample \(X_1,\ldots,X_N\) from a univariate normal distribution, \(Normal(\theta,1)\). It is well known that the sample mean \(\bar{X}\) is unbiased, UMVUE, minimax and admissable. Weirdly enough, there is an estimator which Pitman dominates the sample mean. This example comes from Efron, 1975.Define \(X^*\) by\[ X^* = \bar{X} - \Delta(\bar{X}), \]where \(\Delta\) is an odd function, which for \(x\geq 0\) takes the values,\[ \Delta(x) = \frac{1}{2\sqrt{N}} \min \left\{\sqrt{N}x, \Phi(-\sqrt{N}x)\right\}. \]Then \(X^* \overset{P}{\succ} \bar{X}\).What’s going on? Observe that \(X^*\) is a function of \(\bar{X}\) which shrinks it towards zero. This sounds unintuitive, since the rule dominates the sample mean for any \(\theta\), even when it is very far from 0! But there is actually a similar phenomenon in decision theory, which is known as the Stein effect, in which you may construct an estimator which dominates a minimax estimator by shrinking it towards zero. In a monumental work, Charles Stein proved that this Stein effect occurs when estimating a normal mean vector of length at least 3, when your loss function is the squared-error loss. But no estimator dominates the sample mean in the univariate scenario. The peculiar thing about Pitman domination is you get a Stein-type effect even for estimation of a single normal mean. Even in the simplest possible estimation problem, we end up with a paradoxical result.Pitman Closeness is not transitiveThe next strange property of Pitman closeness is that it is not transitive. In general, it can’t formulate an ordering over a set of decision rules, and as a consequence there may be no Pitman dominant decision rule. This example comes from Robert, 2007.Let \(x \sim Uniform(-0.9\theta,1.1\theta)\). Then consider the decision rules \(\delta_0(x) = x,\delta_1(x)=0.9\mid x \mid\), and \(\delta_2(x) = 3.2 \mid x \mid \). Then \(\delta_0 \overset{P}{\succ}\delta_1,d_1 \overset{P}{\succ}\delta_2,\) and \(d_2 \overset{P}{\succ}\delta_0\).ConclusionPitman closeness is an interesting idea, and was the focus of a fair amount of research at one time. There was a book devoted to it. But today it is a little-known curiosity. Pitman closeness begins with eminently reasonable foundations but then leads to too many conclusions that go against what we might want from a reasonable decision theory. The risk approach to decision theory, despite some it’s own paradoxes, is the accepted benchmark in statisics.</summary></entry><entry><title type="html">The kernel density estimator minimizes the regularized Tsallis score</title><link href="http://localhost:4000/2016/01/25/the-kernel-density-estimator-minimizes-the.html" rel="alternate" type="text/html" title="The kernel density estimator minimizes the regularized Tsallis score" /><published>2016-01-25T13:21:42-05:00</published><updated>2016-01-25T13:21:42-05:00</updated><id>http://localhost:4000/2016/01/25/the-kernel-density-estimator-minimizes-the</id><content type="html" xml:base="http://localhost:4000/2016/01/25/the-kernel-density-estimator-minimizes-the.html">&lt;p&gt;The kernel density estimator (KDE) is a simple and popular tool for nonparametric density estimation. In one-dimension it is given by&lt;/p&gt;&lt;p&gt;\[ \hat{p}_{KDE}(x) = \frac{1}{Nh} \sum_{i=1}^NK\left(\frac{x-X_i}{h}\right). \]&lt;/p&gt;&lt;p&gt;\(K\) is a kernel (let’s say a variance 1 density for simplicity). It has a simple closed form, and there is extensive literature on theoretical justifications for KDE. One conceptual difficulty with KDE is that it is not represented as a solution to an optimization problem. Most statistics and ML algorithms, from PCA to SVM to k-means are either formulated as an optimization or may be alternatively formulated as the solution to one. For me, this gives me better intuition, and often provides a decision-theoretic justification for the problem. For example, the popular ML algorithm &lt;a href=&quot;https://en.wikipedia.org/wiki/AdaBoost#Boosting_as_gradient_descent&quot;&gt;AdaBoost&lt;/a&gt;, the first boosting algorithm, benefited from new insights and extensions when it was &lt;a href=&quot;http://www.jstor.org/stable/2699986&quot;&gt;discovered&lt;/a&gt; that it is essentially a greedy algorithm for optimizing the exponential classification loss.&lt;/p&gt;&lt;p&gt;A few years ago I came across the paper &lt;a href=&quot;http://ysidro.econ.uiuc.edu/~roger/research/densiles/heat.pdf&quot;&gt;What do Kernel Density Estimators Optimize?&lt;/a&gt; by Koenker et al. It has some interesting connections between the &lt;a href=&quot;https://en.wikipedia.org/wiki/Heat_equation&quot;&gt;heat equation&lt;/a&gt; and KDEs, but the theorem I find most interesting is the following:&lt;/p&gt;&lt;p&gt;&lt;b&gt;Theorem 1: &lt;/b&gt;Let \(E_n[\cdot]\) denote the expectation operator with respect to the empirical distribution of a sample \(X_1,\ldots,X_N\). The solution to&lt;/p&gt;&lt;p&gt;\[\min_f \left\{ -\mathbb{E}_n[f(x,\lambda)]+ \frac{1}{2} \intop f(x,\lambda)^2 dx +\frac{\lambda}{2} \intop \left(\frac{\partial f(x,\lambda)}{\partial x}\right)^2 dx \right\}\]&lt;/p&gt;&lt;p&gt;is given by&lt;/p&gt;&lt;p&gt;\[ \hat{p}(x) = \frac{1}{2\sqrt{\lambda n}} \sum_{i=1}^N \exp \{ -\mid x-X_i \mid /\sqrt{\lambda} \}. \]&lt;/p&gt;&lt;p&gt;This is precisely the KDE with a &lt;a href=&quot;https://en.wikipedia.org/wiki/Laplace_distribution&quot;&gt;Laplace kernel&lt;/a&gt;. In section 4 of the paper, the authors show that by changing the third term in the optimization to a different penalty, the solution is a KDE with a different choice of kernel. Like with other &lt;a href=&quot;https://scholar.google.com/scholar?cluster=10372793773687401003&amp;amp;hl=en&amp;amp;as_sdt=0,14&quot;&gt;kernel methods&lt;/a&gt; such as&lt;a href=&quot;https://en.wikipedia.org/wiki/Support_vector_machine&quot;&gt; kernel SVM&lt;/a&gt; , each kernel corresponds to a norm in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space&quot;&gt;reproducing kernel Hilbert space&lt;/a&gt;. Changing the penalty term to that particular norm will result in a kernel estimator with the corresponding kernel for that RKHS. For example, we could change the penalty from &lt;/p&gt;&lt;p&gt;\( \intop \left(\frac{\partial f(x,\lambda)}{\partial x}\right)^2 dx \) to \(

 \intop \left(\frac{\partial^2 f(x,\lambda)}{\partial x^2}\right)^2 dx\) and we would get a KDE with a kernel of the form \(K_\sigma(x)=\frac{1}{2}\exp\{-\mid x/\sigma\mid/\sqrt{2}\}\sin(\mid x/\sigma\mid/\sqrt{2}+\pi/4)/\sigma\).&lt;/p&gt;&lt;p&gt;This is a pretty fascinating result, which says that the KDE &lt;i&gt;is &lt;/i&gt;in fact the solution to an optimization problem. Moreover, this optimization has some deep foundations which I will describe in the next section&lt;/p&gt;&lt;p&gt;&lt;b&gt;Scoring rules&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A &lt;i&gt;&lt;a href=&quot;https://www.researchgate.net/profile/Philip_Dawid/publication/259529033_Theory_and_Applications_of_Proper_Scoring_Rules/links/02e7e53274ba9db6d5000000.pdf&quot;&gt;scoring rule&lt;/a&gt; &lt;/i&gt;is a function \( s(x,Q)\) which measures the accuracy of predicting an observation \(x\) from an unknown distribution \(P\) with a distribution \(Q\). A scoring rule is &lt;i&gt;proper &lt;/i&gt;if the expected score is minimized by the choice of \(Q=P\):&lt;/p&gt;&lt;p&gt;\[ P = \text{argmin}_Q \mathbb{E}_{X\sim P} [s(X,Q)].\]&lt;/p&gt;&lt;p&gt;Every proper scoring rule has a corresponding &lt;i&gt;entropy&lt;/i&gt;:&lt;/p&gt;&lt;p&gt;\[ H(P) = \mathbb{E}_{X\sim P} [s(X,P)], \]&lt;/p&gt;&lt;p&gt;and a &lt;i&gt;divergence&lt;/i&gt;:&lt;/p&gt;&lt;p&gt;\[ D(P,Q) = 

\mathbb{E}_{X\sim P}[ s(X,P)] - 

\mathbb{E}_{X\sim P}[ s(X,Q)], \]&lt;/p&gt;&lt;p&gt;so there are intimate ties between proper scoring rules and information theory. I won’t get into too much detail about proper scoring rules here, I talk about them in-depth in my &lt;a href=&quot;http://arxiv.org/abs/1506.03537&quot;&gt;dissertation&lt;/a&gt;, including examples of several estimators which correspond to optimizing certain proper scoring rules. But in general,  an estimator which optimizes a proper scoring rule is going to be consistent (this can be made rigorous). From the above formula, minimizing a proper scoring rule with respect to the data \( \min_Q\mathbb{E}_n[s(X,Q)]\) is the same as finding a \(Q\) which  minimizes the divergence to the empirical distribution \(\hat{P}\), \(\min_Q D(\hat{P},Q)\). Basically, any proper scoring rule will find an estimate “close” to the empirical distribution of the data, but the measure of closeness will differ by scoring rule.&lt;/p&gt;&lt;p&gt;By far the most common proper scoring rule is the &lt;i&gt;log score&lt;/i&gt;,&lt;/p&gt;&lt;p&gt;\[ l(x,Q) = -\log q(x), \]&lt;/p&gt;&lt;p&gt;

where \(q = Q’\). Minimizing the empirical log score is the same as minimizing the &lt;i&gt;Kullback-Leibler Divergence&lt;/i&gt;, \(KL(P,Q)= \mathbb{E}_P[\log p(X)/q(X)]\). More familiarly, this is equivalent to the ubiquitous &lt;i&gt;maximum likelihood&lt;/i&gt;. &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood#Properties&quot;&gt;Maximum likelihood&lt;/a&gt; is so popular because under mild conditions it has very strong guarantees (asymptotically unbiased, efficient, normal). Other scoring rules may produce a sub-optimal estimator, but there are often situations (such as there being computations limitations) which lead to considering another scoring rule.&lt;br/&gt;&lt;/p&gt;&lt;!-- more --&gt;&lt;p&gt;&lt;b&gt;Tsallis score&lt;/b&gt;&lt;/p&gt;&lt;p&gt;One class of proper scoring rules is the &lt;i&gt;Tsallis score&lt;/i&gt;:&lt;/p&gt;&lt;p&gt;\[ t_{\gamma}(x,Q) = (\gamma-1)\intop q(y)^\gamma dy - \gamma q(x)^{\gamma-1},\]&lt;/p&gt;&lt;p&gt;Up until now the only interesting thing I’ve known about the Tsallis score is it’s entropy is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Tsallis_entropy&quot;&gt;Tsallis entropy&lt;/a&gt; which is used in statistical mechanics. I have never seen it used in statistical inference. One curious thing about the Tsallis score is it is not &lt;i&gt;local&lt;/i&gt;, which means the score at an observation \(x\) also depends on evaluating the density at points besides \(x\) (given by the integral over \(q\)). &lt;/p&gt;&lt;p&gt;Now, by simple observation we can show that the KDE is the solution to&lt;/p&gt;&lt;p&gt;\[ \min_q \left\{ 

\frac{1}{2}

\mathbb{E}_n [ t_2(X,q) ] + \frac{\lambda}{2} R(q) \right\}, \]&lt;/p&gt;&lt;p&gt;where \(R(q)\) is some norm in a reproducing kernel Hilbert space (RKHS). In words, the KDE is the solution to minimizing the Tsallis score of the data, plus some roughness penalty. This shows a close connection to other common density estimators such as the &lt;a href=&quot;http://projecteuclid.org/euclid.aos/1176345872&quot;&gt;penalized maximum likelihood estimator&lt;/a&gt;, which minimizes the penalized &lt;i&gt;Log score:&lt;/i&gt;&lt;/p&gt;&lt;p&gt;\[ \min_q \left\{-\mathbb{E}_n [ \log q(X) ] + \frac{\lambda}{2} R(q)\right\}, \]&lt;/p&gt;&lt;p&gt;with the added constraint that \(\intop q(x) dx =1\).&lt;/p&gt;&lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;/p&gt;&lt;p&gt;There are two interesting insights to be learned here. The first is that the KDE has solid decision-theoretic foundations, though I’ve never seen that expressed. KDE is usually interpreted as a “smoothed histogram”, which is true, but it has deeper justifications. That the KDE pops out as the solution to optimizing the Tsallis score, an arbitrary scoring rule I’ve never seen used before, seems perplexing.&lt;/p&gt;&lt;p&gt;The second insight is that optimizing the regularized Tsallis score gives an estimator with a closed-form. I describe several other regularized scoring rule estimators in my dissertation, but they don’t have such a simple solution. Granted, finding the solution required solving the heat equation PDE, but I wonder if there are more insights and connections to be made using some knowledge of PDEs and variational calculus.&lt;/p&gt;&lt;figure data-orig-width=&quot;500&quot; data-orig-height=&quot;328&quot; class=&quot;tmblr-full&quot;&gt;&lt;img src=&quot;https://66.media.tumblr.com/0290c2394222528ee7a060701b2000d7/tumblr_inline_o1o604C8wn1tlyjch_540.jpg&quot; alt=&quot;image&quot; data-orig-width=&quot;500&quot; data-orig-height=&quot;328&quot;/&gt;&lt;/figure&gt;</content><author><name></name></author><category term="data" /><category term="statistics" /><summary type="html">The kernel density estimator (KDE) is a simple and popular tool for nonparametric density estimation. In one-dimension it is given by\[ \hat{p}_{KDE}(x) = \frac{1}{Nh} \sum_{i=1}^NK\left(\frac{x-X_i}{h}\right). \]\(K\) is a kernel (let’s say a variance 1 density for simplicity). It has a simple closed form, and there is extensive literature on theoretical justifications for KDE. One conceptual difficulty with KDE is that it is not represented as a solution to an optimization problem. Most statistics and ML algorithms, from PCA to SVM to k-means are either formulated as an optimization or may be alternatively formulated as the solution to one. For me, this gives me better intuition, and often provides a decision-theoretic justification for the problem. For example, the popular ML algorithm AdaBoost, the first boosting algorithm, benefited from new insights and extensions when it was discovered that it is essentially a greedy algorithm for optimizing the exponential classification loss.A few years ago I came across the paper What do Kernel Density Estimators Optimize? by Koenker et al. It has some interesting connections between the heat equation and KDEs, but the theorem I find most interesting is the following:Theorem 1: Let \(E_n[\cdot]\) denote the expectation operator with respect to the empirical distribution of a sample \(X_1,\ldots,X_N\). The solution to\[\min_f \left\{ -\mathbb{E}_n[f(x,\lambda)]+ \frac{1}{2} \intop f(x,\lambda)^2 dx +\frac{\lambda}{2} \intop \left(\frac{\partial f(x,\lambda)}{\partial x}\right)^2 dx \right\}\]is given by\[ \hat{p}(x) = \frac{1}{2\sqrt{\lambda n}} \sum_{i=1}^N \exp \{ -\mid x-X_i \mid /\sqrt{\lambda} \}. \]This is precisely the KDE with a Laplace kernel. In section 4 of the paper, the authors show that by changing the third term in the optimization to a different penalty, the solution is a KDE with a different choice of kernel. Like with other kernel methods such as kernel SVM , each kernel corresponds to a norm in a reproducing kernel Hilbert space. Changing the penalty term to that particular norm will result in a kernel estimator with the corresponding kernel for that RKHS. For example, we could change the penalty from \( \intop \left(\frac{\partial f(x,\lambda)}{\partial x}\right)^2 dx \) to \(</summary></entry><entry><title type="html">An anti-central limit theorem</title><link href="http://localhost:4000/2016/01/24/an-anti-central-limit-theorem.html" rel="alternate" type="text/html" title="An anti-central limit theorem" /><published>2016-01-24T21:54:16-05:00</published><updated>2016-01-24T21:54:16-05:00</updated><id>http://localhost:4000/2016/01/24/an-anti-central-limit-theorem</id><content type="html" xml:base="http://localhost:4000/2016/01/24/an-anti-central-limit-theorem.html">&lt;p&gt;The basic version of the central limit theorem says that for an i.i.d. sample from a distribution with finite variance \(\sigma^2\) and mean \(\mu\), the re-scaled sample mean converges to a standard normal distribution:&lt;/p&gt;&lt;p&gt;\[ \sqrt{N} (\bar{X} - \mu) \rightarrow Normal(0,\sigma^2). \]&lt;/p&gt;&lt;p&gt;Basically the CLT says that the error in the sample mean in estimating the location parameter decreases at a rate of \(O(N^{-&amp;frac12;})\) with the sample size. The CLT is what makes statistical inference possible. Some parts of the above statement may be relaxed, but if you get rid of the assumption of finite variance, things can get weird.&lt;/p&gt;&lt;p&gt;The Cauchy distribution with location \(\mu\) and scale \(\sigma\) is a distribution on \((-\infty,\infty)\) with density&lt;/p&gt;&lt;p&gt;\[ f(x) = \frac{1}{\sigma\pi(1+(\frac{x-\mu}{\sigma})^2}. \]&lt;/p&gt;&lt;p&gt;It is well-known that the sample mean \(\bar{X}\) of an i.i.d. sample \(X_1,\ldots,X_N\) from a Cauchy distribution has the same law as a single sample, \(\bar{X}\overset{D}{=} X_1\). And so the sample mean does not improve as an estimate of the location as the number of samples increases.&lt;/p&gt;&lt;p&gt;A surprising fact is there are distributions where the sample mean gets &lt;i&gt;worse &lt;/i&gt;as an estimate of location as the number of samples increases.&lt;/p&gt;&lt;p&gt;Let \(X\sim Normal(0,1)\), and let \(Y=\frac{1}{X^2}\). Applying a change of variables we find that \(Y\) has distribution function&lt;/p&gt;&lt;p&gt;\[ F(y) = 2\left(1-\Phi\left(\frac{1}{\sqrt{y}}\right)\right), \]&lt;/p&gt;&lt;p&gt;where \(\Phi\) is the standard normal c.d.f.&lt;/p&gt;&lt;p&gt;Define \(F_a(y) = F(y/a)\) to be the scale family of densities for \(F\). Consider i.i.d. \(Y_1,Y_2\sim F=F_1\). It is possible to verify that the convolution \(Y_1+Y_2 \sim F_2\) (I will leave that as an exercise).&lt;/p&gt;&lt;p&gt;What does this tell us? Consider the probability&lt;/p&gt;&lt;p&gt;\[ P\left(\frac{Y_1+Y_2}{4} \leq y\right)  = 2 \left(1 - \Phi\left(\frac{2}{(4y)^{&amp;frac12;}}\right) \right)= F_1(y).\]&lt;/p&gt;&lt;p&gt;Thus, the law of the sample mean with two samples has the same law as &lt;i&gt;twice&lt;/i&gt; any of the individual samples.&lt;/p&gt;&lt;p&gt;In general, we get that the sample mean has the same law as a sample &lt;i&gt;scaled by the number of samples&lt;/i&gt;:&lt;br/&gt;&lt;/p&gt;&lt;p&gt;\[ \bar{Y} \overset{D}{=} N Y_1. \]&lt;/p&gt;&lt;p&gt;Instead of the error decreasing, the the error of the sample mean in estimating the location &lt;i&gt;increases at a rate of \(O(N)\) &lt;/i&gt;with the number of samples!&lt;/p&gt;&lt;p&gt;Postscript: This distribution is called the &lt;a href=&quot;https://en.wikipedia.org/wiki/L%C3%A9vy_distribution&quot;&gt;Lévy&lt;/a&gt; distribution and the unusual behavior of the sample mean (for Cauchy as well) is described by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Stable_distribution#A_generalized_central_limit_theorem&quot;&gt;generalized central limit theorem&lt;/a&gt;. The Lévy, Cauchy and Gaussian distribution are all &lt;i&gt;stable &lt;/i&gt;distributions, which means the families are each closed under averaging. For the Gaussian family, the law of the sample mean follows \(\bar{X}\overset{D}{=} N^{-&amp;frac12;} X_1\).&lt;/p&gt;</content><author><name></name></author><summary type="html">The basic version of the central limit theorem says that for an i.i.d. sample from a distribution with finite variance \(\sigma^2\) and mean \(\mu\), the re-scaled sample mean converges to a standard normal distribution:\[ \sqrt{N} (\bar{X} - \mu) \rightarrow Normal(0,\sigma^2). \]Basically the CLT says that the error in the sample mean in estimating the location parameter decreases at a rate of \(O(N^{-&amp;frac12;})\) with the sample size. The CLT is what makes statistical inference possible. Some parts of the above statement may be relaxed, but if you get rid of the assumption of finite variance, things can get weird.The Cauchy distribution with location \(\mu\) and scale \(\sigma\) is a distribution on \((-\infty,\infty)\) with density\[ f(x) = \frac{1}{\sigma\pi(1+(\frac{x-\mu}{\sigma})^2}. \]It is well-known that the sample mean \(\bar{X}\) of an i.i.d. sample \(X_1,\ldots,X_N\) from a Cauchy distribution has the same law as a single sample, \(\bar{X}\overset{D}{=} X_1\). And so the sample mean does not improve as an estimate of the location as the number of samples increases.A surprising fact is there are distributions where the sample mean gets worse as an estimate of location as the number of samples increases.Let \(X\sim Normal(0,1)\), and let \(Y=\frac{1}{X^2}\). Applying a change of variables we find that \(Y\) has distribution function\[ F(y) = 2\left(1-\Phi\left(\frac{1}{\sqrt{y}}\right)\right), \]where \(\Phi\) is the standard normal c.d.f.Define \(F_a(y) = F(y/a)\) to be the scale family of densities for \(F\). Consider i.i.d. \(Y_1,Y_2\sim F=F_1\). It is possible to verify that the convolution \(Y_1+Y_2 \sim F_2\) (I will leave that as an exercise).What does this tell us? Consider the probability\[ P\left(\frac{Y_1+Y_2}{4} \leq y\right)  = 2 \left(1 - \Phi\left(\frac{2}{(4y)^{&amp;frac12;}}\right) \right)= F_1(y).\]Thus, the law of the sample mean with two samples has the same law as twice any of the individual samples.In general, we get that the sample mean has the same law as a sample scaled by the number of samples:\[ \bar{Y} \overset{D}{=} N Y_1. \]Instead of the error decreasing, the the error of the sample mean in estimating the location increases at a rate of \(O(N)\) with the number of samples!Postscript: This distribution is called the Lévy distribution and the unusual behavior of the sample mean (for Cauchy as well) is described by the generalized central limit theorem. The Lévy, Cauchy and Gaussian distribution are all stable distributions, which means the families are each closed under averaging. For the Gaussian family, the law of the sample mean follows \(\bar{X}\overset{D}{=} N^{-&amp;frac12;} X_1\).</summary></entry><entry><title type="html">A random variable is not uniquely determined by its moments</title><link href="http://localhost:4000/2016/01/24/a-random-variable-is-not-uniquely-determined-by.html" rel="alternate" type="text/html" title="A random variable is not uniquely determined by its moments" /><published>2016-01-24T10:50:58-05:00</published><updated>2016-01-24T10:50:58-05:00</updated><id>http://localhost:4000/2016/01/24/a-random-variable-is-not-uniquely-determined-by</id><content type="html" xml:base="http://localhost:4000/2016/01/24/a-random-variable-is-not-uniquely-determined-by.html">&lt;p&gt;The &lt;i&gt;moments &lt;/i&gt;of a random variable \(X\) are given by \(\mathbb{E}[X^n]\), for all integers \(n\geq 1\). One fascinating fact I first learned while studying distribution theory is that the moment sequence does not always uniquely determine a distribution.&lt;/p&gt;&lt;p&gt;Consider \(X\sim logNormal(0,1)\), that is, \(log(X)\) follows a standard normal distribution, and let the density of \(X\) be \(f(x)\). The moments of \(X\) exist and have the closed form&lt;/p&gt;&lt;p&gt;\[ m_i := \mathbb{E}[X^i] = e^{i^2/2}. \]&lt;/p&gt;&lt;p&gt;Consider the density&lt;/p&gt;&lt;p&gt;\[ f_a(x) = f(x)(1+\sin(2\pi\log(x))), \,\,\, x\geq 0. \]&lt;/p&gt;&lt;p&gt;Then \(f,f_a\) have the same moments. To prove this, it’s sufficient to show that for each \(i=1,2,\ldots\),&lt;/p&gt;&lt;p&gt;\[ \intop_{0}^\infty x^i f(x) \sin(2\pi\log(x)) dx =0 .\]&lt;/p&gt;&lt;p&gt;This can be verified by applying the change-of-variables \(y=\log x\) to the integral and showing the resulting integral is over an odd function on the real line.&lt;/p&gt;&lt;figure data-orig-width=&quot;534&quot; data-orig-height=&quot;443&quot; class=&quot;tmblr-full&quot;&gt;&lt;img src=&quot;https://66.media.tumblr.com/b4bf1c6bdbb8c9b3cd2d59362a35061f/tumblr_inline_o1grspsXys1tlyjch_540.png&quot; alt=&quot;image&quot; data-orig-width=&quot;534&quot; data-orig-height=&quot;443&quot;/&gt;&lt;/figure&gt;&lt;p&gt;Figure 1: Two densities with the same moments&lt;/p&gt;&lt;p&gt;&lt;b&gt;When do the moments determine the distribution?&lt;/b&gt;&lt;/p&gt;&lt;p&gt;One sufficient condition for moments uniquely determining the distribution is the &lt;i&gt;Carleman condition&lt;/i&gt;:&lt;/p&gt;&lt;p&gt;\[ \sum_{i=1}^\infty m_{2i}^{-1/(2i)} = \infty. \]&lt;/p&gt;&lt;p&gt;You can verify that the lognormal distribution has moments which grow too quickly for this Condition to hold.&lt;/p&gt;</content><author><name></name></author><summary type="html">The moments of a random variable \(X\) are given by \(\mathbb{E}[X^n]\), for all integers \(n\geq 1\). One fascinating fact I first learned while studying distribution theory is that the moment sequence does not always uniquely determine a distribution.Consider \(X\sim logNormal(0,1)\), that is, \(log(X)\) follows a standard normal distribution, and let the density of \(X\) be \(f(x)\). The moments of \(X\) exist and have the closed form\[ m_i := \mathbb{E}[X^i] = e^{i^2/2}. \]Consider the density\[ f_a(x) = f(x)(1+\sin(2\pi\log(x))), \,\,\, x\geq 0. \]Then \(f,f_a\) have the same moments. To prove this, it’s sufficient to show that for each \(i=1,2,\ldots\),\[ \intop_{0}^\infty x^i f(x) \sin(2\pi\log(x)) dx =0 .\]This can be verified by applying the change-of-variables \(y=\log x\) to the integral and showing the resulting integral is over an odd function on the real line.Figure 1: Two densities with the same momentsWhen do the moments determine the distribution?One sufficient condition for moments uniquely determining the distribution is the Carleman condition:\[ \sum_{i=1}^\infty m_{2i}^{-1/(2i)} = \infty. \]You can verify that the lognormal distribution has moments which grow too quickly for this Condition to hold.</summary></entry><entry><title type="html">The sample mean is a sub-optimal estimate of the population mean</title><link href="http://localhost:4000/2016/01/17/the-sample-mean-is-a-sub-optimal-estimate-of-the.html" rel="alternate" type="text/html" title="The sample mean is a sub-optimal estimate of the population mean" /><published>2016-01-17T09:31:25-05:00</published><updated>2016-01-17T09:31:25-05:00</updated><id>http://localhost:4000/2016/01/17/the-sample-mean-is-a-sub-optimal-estimate-of-the</id><content type="html" xml:base="http://localhost:4000/2016/01/17/the-sample-mean-is-a-sub-optimal-estimate-of-the.html">&lt;p&gt;It may be surprising that the sample mean is not usually the best estimate of the population mean. It’s well known that when the data is Gaussian, the sample mean is the &lt;i&gt;UMVUE&lt;/i&gt;, or the uniformly minimum variance unbiased estimator. The UMVUE has the smallest variance among all unbiased estimators for &lt;i&gt;any &lt;/i&gt;choice of the unknown parameter \( \theta \).  The sample mean is always unbiased (when the population mean exists) but isn’t generally minimum variance, and in some cases can have a much higher variance than optimal. This general result was first discovered by Kagan, Linnik and Rao (1965). This derivation comes from Székely et al., 1986.&lt;/p&gt;&lt;p&gt;Let \(F\) be a mean-zero, finite variance distribution function, and let \(F_{\theta}(x) = F(x-\theta)\). \(\theta\) is known as the &lt;i&gt;location parameter, &lt;/i&gt;and in this setup it is the same as the mean of the distribution. We denote the density \( f(x) = F(x)’\).&lt;/p&gt;&lt;p&gt;&lt;b&gt;An Example&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Suppose that the base distribution is \(\text{Uniform(-1,1)}\), so that \( f(x) = \frac{1}{2} 1 \left\{x\in\{-1,1\}\right\}\). We have \(N\) independent and identically distributed samples from the distribution \(X_1,\ldots,X_N\). Consider the estimator&lt;/p&gt;&lt;p&gt;\( X^* = \frac{1}{2}(X_{(1)}+X_{(N)}),\)&lt;/p&gt;&lt;p&gt;that is, the average of the smallest and largest point in the sample. This is called the &lt;i&gt;midrange&lt;/i&gt;. \(X^*\) may be shown to have variance&lt;/p&gt;&lt;p&gt;\( \text{var}(X^*) = \frac{2}{(N+1)(N+2)}, \)&lt;/p&gt;&lt;p&gt;while the sample mean \( \bar{X} \) has variance&lt;/p&gt;&lt;p&gt;\( \text{var}(\bar{X}) = \frac{1}{3N}, \)&lt;/p&gt;&lt;p&gt;which is substantially larger. \(X^*\) has variance of order \( O(N^{-2}) \), which is a whole order of magnitude smaller than that of the sample mean.&lt;/p&gt;&lt;p&gt;&lt;b&gt;The main result&lt;/b&gt;&lt;/p&gt;&lt;p&gt;We will now construct a statistic which is &lt;i&gt;asymptotically UMVUE&lt;/i&gt;: it is asymptotally unbiased and minimum variance among all estimators of the population mean for data arising from a particular location family.&lt;/p&gt;&lt;p&gt;Consider the estimator&lt;/p&gt;&lt;p&gt;\( X^* = \sum_{i=1}^N a_{i,N} X_{(i)}, \)&lt;/p&gt;&lt;p&gt;where \(X_{(i)}\) is the i-th order statistic of the sample, and \(a_{i,N}\) are weights which depend on the distribution.&lt;/p&gt;&lt;p&gt;Suppose the distribution function \(F\) has three derivatives, let \(f(x) = F(x)’\),  and define the functional&lt;br/&gt;&lt;/p&gt;&lt;p&gt;\[ a(F(x)) = - \left(\left( A + Bx \right)\left(\log f(x)\right)’ \right)’, \]&lt;/p&gt;&lt;p&gt;where&lt;/p&gt;&lt;p&gt;\(\begin{align} A &amp;amp;= \frac{\mu_2}{\mu_0\mu_2 - \mu_1^2},  &amp;amp; B = \frac{\mu_1}{\mu_0\mu_2 - \mu_1^2},  \end{align}\)&lt;/p&gt;&lt;p&gt;and &lt;/p&gt;&lt;p&gt;\( \begin{align} \mu_0 &amp;amp;= \intop \frac{f’(x)^2}{f(x)} dx,   &amp;amp; \mu_1 = \intop x\frac{f’(x)^2}{f(x)} dx,\end{align}\)&lt;br/&gt;&lt;/p&gt;&lt;p&gt;\(  \mu_2 = \intop x^2\frac{f’(x)^2}{f(x)} dx - 1, \)&lt;/p&gt;&lt;p&gt;then with the choice \(a_{i,N} = a(i/N)/N \), \(X^*\) is asymptotically UMVUE.&lt;/p&gt;</content><author><name></name></author><summary type="html">It may be surprising that the sample mean is not usually the best estimate of the population mean. It’s well known that when the data is Gaussian, the sample mean is the UMVUE, or the uniformly minimum variance unbiased estimator. The UMVUE has the smallest variance among all unbiased estimators for any choice of the unknown parameter \( \theta \).  The sample mean is always unbiased (when the population mean exists) but isn’t generally minimum variance, and in some cases can have a much higher variance than optimal. This general result was first discovered by Kagan, Linnik and Rao (1965). This derivation comes from Székely et al., 1986.Let \(F\) be a mean-zero, finite variance distribution function, and let \(F_{\theta}(x) = F(x-\theta)\). \(\theta\) is known as the location parameter, and in this setup it is the same as the mean of the distribution. We denote the density \( f(x) = F(x)’\).An ExampleSuppose that the base distribution is \(\text{Uniform(-1,1)}\), so that \( f(x) = \frac{1}{2} 1 \left\{x\in\{-1,1\}\right\}\). We have \(N\) independent and identically distributed samples from the distribution \(X_1,\ldots,X_N\). Consider the estimator\( X^* = \frac{1}{2}(X_{(1)}+X_{(N)}),\)that is, the average of the smallest and largest point in the sample. This is called the midrange. \(X^*\) may be shown to have variance\( \text{var}(X^*) = \frac{2}{(N+1)(N+2)}, \)while the sample mean \( \bar{X} \) has variance\( \text{var}(\bar{X}) = \frac{1}{3N}, \)which is substantially larger. \(X^*\) has variance of order \( O(N^{-2}) \), which is a whole order of magnitude smaller than that of the sample mean.The main resultWe will now construct a statistic which is asymptotically UMVUE: it is asymptotally unbiased and minimum variance among all estimators of the population mean for data arising from a particular location family.Consider the estimator\( X^* = \sum_{i=1}^N a_{i,N} X_{(i)}, \)where \(X_{(i)}\) is the i-th order statistic of the sample, and \(a_{i,N}\) are weights which depend on the distribution.Suppose the distribution function \(F\) has three derivatives, let \(f(x) = F(x)’\),  and define the functional\[ a(F(x)) = - \left(\left( A + Bx \right)\left(\log f(x)\right)’ \right)’, \]where\(\begin{align} A &amp;amp;= \frac{\mu_2}{\mu_0\mu_2 - \mu_1^2},  &amp;amp; B = \frac{\mu_1}{\mu_0\mu_2 - \mu_1^2},  \end{align}\)and \( \begin{align} \mu_0 &amp;amp;= \intop \frac{f’(x)^2}{f(x)} dx,   &amp;amp; \mu_1 = \intop x\frac{f’(x)^2}{f(x)} dx,\end{align}\)\(  \mu_2 = \intop x^2\frac{f’(x)^2}{f(x)} dx - 1, \)then with the choice \(a_{i,N} = a(i/N)/N \), \(X^*\) is asymptotically UMVUE.</summary></entry><entry><title type="html">Clustering</title><link href="http://localhost:4000/2016/01/13/clustering.html" rel="alternate" type="text/html" title="Clustering" /><published>2016-01-13T22:22:33-05:00</published><updated>2016-01-13T22:22:33-05:00</updated><id>http://localhost:4000/2016/01/13/clustering</id><content type="html" xml:base="http://localhost:4000/2016/01/13/clustering.html">&lt;p&gt;Clustering is one of the fundamental tasks in unsupervised learning, but there are a huge diversity of approaches. There is &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering&quot;&gt;k-means&lt;/a&gt; , &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_shift&quot;&gt;mean-shift&lt;/a&gt;,&lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_clustering&quot;&gt; hierarchical clustering&lt;/a&gt;,&lt;a href=&quot;https://en.wikipedia.org/wiki/Mixture_model&quot;&gt; mixture models&lt;/a&gt; and more. The reason for so many varied approaches is that clustering by itself is ill-defined. In this post I will focus on two methods, mean-shift and Gaussian mixture modeling, because they have a more “statistical” flavor, in that they can be related to modeling a probability distribution over the data. Despite their similarities, they are based on sometimes contradictory principles.&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Mean-shift&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Mean-shift clusters points by the modes of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_density_estimation&quot;&gt;nonparametric kernel density&lt;/a&gt;. The kernel density estimator for data \(x_1,\ldots,x_N\) is&lt;/p&gt;&lt;p&gt;\(\hat{p}(x) =\frac{1}{nh^d} \sum_{i=1}^NK(\frac{\Vert x-x_i\Vert^2}{h}),\)&lt;/p&gt;&lt;p&gt;where \(K\) is a kernel function. You can imagine the kernel to be some density with variance one, such as a Gaussian density, with variability controlled by the bandwidth \(h\). In effect a kernel density is a smoothed histogram, a mixture of local densities at each datum. \(\hat{p}\) has some number of modes, let’s say \(p \). &lt;/p&gt;&lt;p&gt;The gradient of the density at a point \(x\) looks like&lt;/p&gt;&lt;p&gt;\( \nabla \hat{p}(x) = L(x)\cdot\left( \frac{ \sum_{i=1}^N x_i k(\frac{\Vert x-x_i\Vert^2}{h})}{\sum_{i=1}^n k(\frac{\Vert x-x_i\Vert^2}{h})} - x \right), \)&lt;/p&gt;&lt;p&gt;where \(L(x)\) is a positive function, \(k=K’\) and the second term is called the mean-shift, which we denote \(m(x)\). Any mode of \(\hat{p}\) will satisfy the fixed point \(m(x)=0\).&lt;/p&gt;&lt;p&gt;Mean-shift works as follows. Initialize \(w_1\) at some point. Then iterate the following until convergence:&lt;/p&gt;&lt;p&gt;\(w_t \leftarrow w_{t-1} + m(w_{t-1}).\)&lt;/p&gt;&lt;p&gt;Each iteration moves the point in an ascent direction of the density. Eventually it will converge to a fixed point, which is one of the modes of \(\hat{p}\). We do this for each data point, and we assign clusters based on which fixed point it converged to.&lt;/p&gt;&lt;p&gt;It’s well known that the kernel density is a consistent estimate of the true density for large sample size (and further, it’s modes are consistent as well). But there’s not a lot of understanding on guarantees for mean-shift itself. Also, density estimation is sensitive to bandwidth, and the number of modes can change for different choices of \(h\).&lt;/p&gt;&lt;!-- more --&gt;&lt;p&gt;&lt;b&gt;2. Mixture Models&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Mixture models cluster based on a most probable assignment to a mixture component. It provides a generative interpretation for the data. It can do “soft clustering”, which means it provides not just the most probable cluster assignment, but the full probability vector (known as the responsibilities) for a datum belonging to all the clusters. In addition to clustering you also get a predictive density at no extra cost.&lt;/p&gt;

&lt;p&gt;We may think of a Gaussian mixture model as a latent variable model. Suppose we have a random vector \(Y \sim Multinomial(1,p)\), that is \(Y\) chooses one value of \(\{1,\ldots,D\}\) according to probabilities \(p_1,\ldots,p_D\). Furthermore, for \(j\in \{1,\ldots,D\}\), we suppose \(Z_j \sim Normal(\mu_j,\Sigma_j)\), with each \(Z\) mutually independent and independent of \(Y\).  A random vector \(X\) is a Gaussian mixture when we write it as.&lt;/p&gt;&lt;p&gt;\( X = \sum_{j=1}^D Y_j Z_j .\)&lt;/p&gt;&lt;p&gt;The density for a D-component GMM is given by&lt;/p&gt;

&lt;p&gt;\(p(x \mid \{\theta_j, \mu_j,\Sigma_j\}_{j=1}^D ) = \sum_{j=1}^D \theta_j N(x \mid \mu_j,\Sigma_j), \)&lt;/p&gt;

&lt;p&gt;where \(N(x \mid \mu,\Sigma)\) is a normal density. We maximize the log-likelihood:&lt;/p&gt;

&lt;p&gt;\( \max_{\left\{\{\theta_j, \mu_j,\Sigma_j\}_{j=1}^D\right\}} \sum_{i=1}^N \log 

p(x \mid \theta, \{\mu_j,\Sigma_j\}_{j=1}^D ) 

. \)&lt;/p&gt;

&lt;p&gt;This can be treated as a standard smooth optimization and use any solver of choice, but it’s more common to use the EM-algorithm.&lt;br/&gt;&lt;/p&gt;&lt;p&gt;The optimization of a GMM has some peculiar characteristics. Firstly, the likelihood has singularities. Suppose one of the clusters has a mean equal to one of the data. As the variance of the cluster goes to zero (so that one of the clusters becomes a point mass at the datum), the overall likelihood becomes unbounded. In such cases we say the maximum likelihood does not exist, but in practice we can often get a good solution which corresponds to some local optimum if we start at a reasonable initialization. Another phenomenon with mixture models is the “label switching” problem: there are always D! equivalent solutions corresponding to permutations of the parameter values among the clusters.&lt;/p&gt;&lt;p&gt;For a Gaussian mixture model, the number of modes of the density may be more or less than the number of clusters [1]. Also, the modes will generally not agree with the component means. Mixture modeling and mean-shift are really solutions to two different clustering objectives.&lt;/p&gt;&lt;figure data-orig-width=&quot;662&quot; data-orig-height=&quot;397&quot; class=&quot;tmblr-full&quot;&gt;&lt;img src=&quot;https://66.media.tumblr.com/98e837eb472077587e3ceb60adc6cff5/tumblr_inline_o0x85dEJ5v1tlyjch_540.png&quot; alt=&quot;image&quot; data-orig-width=&quot;662&quot; data-orig-height=&quot;397&quot;/&gt;&lt;/figure&gt;&lt;p&gt;

Figure 1: Left: a two-component mixture with one mode; Right: a two-component mixture with three modes.

&lt;br/&gt;&lt;/p&gt;&lt;p&gt;We still have the issue of selecting the number of clusters. This  is similar to choosing the bandwidth in mean-shift. Increasing D will always increase the log-likelihood objective, so you must select using a penalized score like BIC. However, BIC is designed to find a “good” model in the sense of density estimation. This is not necessarily the same as a “good” clustering, which we explained earlier is not well defined.&lt;/p&gt;&lt;p&gt;There are several good implementations of GMM, such as &lt;a href=&quot;http://www.stat.washington.edu/mclust/&quot;&gt;mclust&lt;/a&gt; in R. There is also an &lt;a href=&quot;http://spark.apache.org/docs/latest/mllib-clustering.html#gaussian-mixture&quot;&gt;implementation&lt;/a&gt; for distributed data in Apache Spark MLLIB.&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. Demo&lt;/b&gt;&lt;/p&gt;&lt;p&gt;To demonstrate I fit the Gaussian mixture model in mclust to the &lt;a href=&quot;http://www.stat.cmu.edu/~larry/all-of-statistics/=data/faithful.dat&quot;&gt;faithful&lt;/a&gt; data set, a classic set of measurements of eruption/waiting time pairs at Old Faithful. mclust compares various types of models (spherical, elliptical, equal or different variances, etc.). Here BIC chooses 3 ellipsoidal Gaussian components. Two components are close together, resulting in many points having a high degree of classification uncertainty. &lt;/p&gt;&lt;figure data-orig-width=&quot;762&quot; data-orig-height=&quot;551&quot; class=&quot;tmblr-full&quot;&gt;&lt;img src=&quot;https://66.media.tumblr.com/71c8cd03585fa4817c5a23c75bd41431/tumblr_inline_o0x92a2VeH1tlyjch_540.png&quot; alt=&quot;image&quot; data-orig-width=&quot;762&quot; data-orig-height=&quot;551&quot;/&gt;&lt;/figure&gt;&lt;p&gt;Figure 2: GMM Clustering on faithful data set&lt;/p&gt;&lt;p&gt;Next I try mean-shift using the R package LCPM. Using the bandwidth chosen by something called “self-coverage”. We see the algorithm converges to two modes.&lt;/p&gt;&lt;figure data-orig-width=&quot;859&quot; data-orig-height=&quot;574&quot; class=&quot;tmblr-full&quot;&gt;&lt;img src=&quot;https://66.media.tumblr.com/aad695b30f0e17c0d598be0cd24d008d/tumblr_inline_o107vc5SjK1tlyjch_540.png&quot; alt=&quot;image&quot; data-orig-width=&quot;859&quot; data-orig-height=&quot;574&quot;/&gt;&lt;/figure&gt;&lt;p&gt;Figure 3: Mean-shift gradient curves for faithful data set.&lt;/p&gt;&lt;p&gt;Both of these methods give results that look a little funny. It may be surprising that GMM chose three clusters instead of two. Mean-shift chooses two clusters, but a few of the cluster assignments look surprising. One interesting property of mean-shift is a data point need not be assigned to the closest cluster center (according to some distance), because a mode may have a very large basin of attraction in the density. The cluster center is also not the literal “center” of the data in that cluster.  GMM has three clusters but with a lot of classification uncertainty between two of them, while mean-shift classifies some points to the larger cluster which GMM assigned very little uncertainty of being in the smaller cluster! Also observe that the GMM density, despite having three mixture components, only has two modes. We should appreciate that these methods are really approaching clustering with two possibly incompatible philosophies.&lt;/p&gt;&lt;p&gt;I hope this shows the challenges in clustering even for the simplest of datasets. &lt;/p&gt;&lt;p&gt;There are some Bayesian treatments of Gaussian mixtures that I really like, which overcome several of the limitations of the aforementioned approach. I am working on a project to implement them in &lt;a href=&quot;https://github.com/geb5101h/BayesGaussianMixture&quot;&gt;Spark&lt;/a&gt;. I will do a separate post on the details&lt;/p&gt;

&lt;p&gt;[1]:&lt;a href=&quot;http://faculty2.ucmerced.edu/mcarreira-perpinan/papers/sst03.pdf&quot;&gt;On the Number of Modes of a Gaussian Mixture&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">Clustering is one of the fundamental tasks in unsupervised learning, but there are a huge diversity of approaches. There is k-means , mean-shift, hierarchical clustering, mixture models and more. The reason for so many varied approaches is that clustering by itself is ill-defined. In this post I will focus on two methods, mean-shift and Gaussian mixture modeling, because they have a more “statistical” flavor, in that they can be related to modeling a probability distribution over the data. Despite their similarities, they are based on sometimes contradictory principles.1. Mean-shiftMean-shift clusters points by the modes of a nonparametric kernel density. The kernel density estimator for data \(x_1,\ldots,x_N\) is\(\hat{p}(x) =\frac{1}{nh^d} \sum_{i=1}^NK(\frac{\Vert x-x_i\Vert^2}{h}),\)where \(K\) is a kernel function. You can imagine the kernel to be some density with variance one, such as a Gaussian density, with variability controlled by the bandwidth \(h\). In effect a kernel density is a smoothed histogram, a mixture of local densities at each datum. \(\hat{p}\) has some number of modes, let’s say \(p \). The gradient of the density at a point \(x\) looks like\( \nabla \hat{p}(x) = L(x)\cdot\left( \frac{ \sum_{i=1}^N x_i k(\frac{\Vert x-x_i\Vert^2}{h})}{\sum_{i=1}^n k(\frac{\Vert x-x_i\Vert^2}{h})} - x \right), \)where \(L(x)\) is a positive function, \(k=K’\) and the second term is called the mean-shift, which we denote \(m(x)\). Any mode of \(\hat{p}\) will satisfy the fixed point \(m(x)=0\).Mean-shift works as follows. Initialize \(w_1\) at some point. Then iterate the following until convergence:\(w_t \leftarrow w_{t-1} + m(w_{t-1}).\)Each iteration moves the point in an ascent direction of the density. Eventually it will converge to a fixed point, which is one of the modes of \(\hat{p}\). We do this for each data point, and we assign clusters based on which fixed point it converged to.It’s well known that the kernel density is a consistent estimate of the true density for large sample size (and further, it’s modes are consistent as well). But there’s not a lot of understanding on guarantees for mean-shift itself. Also, density estimation is sensitive to bandwidth, and the number of modes can change for different choices of \(h\).2. Mixture ModelsMixture models cluster based on a most probable assignment to a mixture component. It provides a generative interpretation for the data. It can do “soft clustering”, which means it provides not just the most probable cluster assignment, but the full probability vector (known as the responsibilities) for a datum belonging to all the clusters. In addition to clustering you also get a predictive density at no extra cost.</summary></entry></feed>
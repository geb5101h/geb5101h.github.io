<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Eric Janofsky</title>
  <meta name="description" content="">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/posts/2/">
  
  
  <link rel="alternate" type="application/rss+xml" title="Eric Janofsky" href="http://localhost:4000/feed.xml">

  

  
  <meta property="og:title" content="Eric Janofsky">
  <meta property="og:site_name" content="Eric Janofsky">
  <meta property="og:url" content="http://localhost:4000/posts/2/">
  <meta property="og:description" content="">
  
  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="Eric Janofsky">
  <meta name="twitter:description" content="">
  
  

  <link rel="dns-prefetch" href="https://fonts.gstatic.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css?family=Bitter:400,400i,700&amp;display=swap" rel="stylesheet">

  
  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-156801543-1', 'auto');
    ga('send', 'pageview');

  </script>


</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Eric Janofsky</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/archives/">Archives</a>
      
        
        <a class="page-link" href="https://github.com/geb5101h">GitHub</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home">

  

  

  <ul class="post-list">
    
      
      

      <li>
        <header class="post-header">
    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
          <h1 class="post-title">
            
              <a class="post-link" href="/2016/02/11/a-clt-for-the-differential-entropy-of/">A CLT For the Differential entropy of high-dimensional Gaussian Distributions</a>
            
          </h1>

          <p class="post-meta">
            Feb 11, 2016
            
            
          </p>
        </header>

        <div class="post-content">
          <p>In graduate school most of my research was broadly in the area of multivariate analysis, of which covariance estimation is an important subject. I gave a seminar talk on <a href="http://arxiv.org/pdf/1309.0482.pdf">this</a> paper when I was in grad school. Often results in random matrix theory can be quite complicated, but the proofs for this problem are surprisingly elegant.</p><p>The differential entropy is defined for a density \(p\) as</p><p>\[ H(p) = -\mathbb{E}_p[\log p(X)] . \]</p><p>For a \(D-\)dimensional Gaussian \(N(\mu,\Sigma)\), this is given by the formula</p><p>\[ H(p) = \frac{D}{2}+\frac{D\log (2\pi)}{2} +\frac{\log \mid \Sigma \mid}{2},\]</p><p>where \( \mid \cdot \mid\) denotes the determinant. So for the Gaussian problem, estimating entropy amounts to estimating the log-determinant of the covariance matrix. Note that one representation for the log-determinant is as the sum of the log-eigenvalues:</p><p>\[ \log \mid \hat{\Sigma} \mid = \sum_i \log \lambda_i .\]</p>


        </div>
        
          <p class="post-continue">
            <a href="/2016/02/11/a-clt-for-the-differential-entropy-of/">Read on &rarr;</a>
          </p>
        
      </li>
    
      
      

      <li>
        <header class="post-header">
    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
          <h1 class="post-title">
            
              <a class="post-link" href="/2016/02/02/pitman-closeness-a-strange-alternative-to-risk/">Pitman Closeness, a strange alternative to risk</a>
            
          </h1>

          <p class="post-meta">
            Feb 2, 2016
            
            
          </p>
        </header>

        <div class="post-content">
          <p>Here is the curious story of a one-time alternative to the accepted notions of statistical optimality. Today, when we talk about decision theory, we think of the <i>risk</i>, the expected loss of a particular decision rule. However at one point in the history of Statistics, there was another candidate. Pitman Closeness makes a lot of sense conceptually, and generated quite a bit of interest in past decades. However, it can lead you to some strange conclusions. As such, it has not lasted the test of time.</p><p>Statistical decision theory begins by considering an observation \(x\) drawn from a distribution \(F(x\mid \theta)\) parametrized by \(\theta\), a <i>decision rule </i>\(\delta\) which is a measurable function of the data \(x\), and a<i> loss function</i> \(L(\theta,\delta(x))\), which measures the loss from taking some action \(\delta\). The <i>risk </i>is defined as<br/></p>


        </div>
        
          <p class="post-continue">
            <a href="/2016/02/02/pitman-closeness-a-strange-alternative-to-risk/">Read on &rarr;</a>
          </p>
        
      </li>
    
      
      

      <li>
        <header class="post-header">
    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
          <h1 class="post-title">
            
              <a class="post-link" href="/2016/01/25/the-kernel-density-estimator-minimizes-the/">The kernel density estimator minimizes the regularized Tsallis score</a>
            
          </h1>

          <p class="post-meta">
            Jan 25, 2016
            
              

 •
  
    
    
      
        <a href="/tags/data/">data</a>,
      
    
      
    
  
    
    
      
    
      
        <a href="/tags/statistics/">statistics</a>
      
    
  


            
            
          </p>
        </header>

        <div class="post-content">
          <p>The kernel density estimator (KDE) is a simple and popular tool for nonparametric density estimation. In one-dimension it is given by</p><p>\[ \hat{p}_{KDE}(x) = \frac{1}{Nh} \sum_{i=1}^NK\left(\frac{x-X_i}{h}\right). \]</p><p>\(K\) is a kernel (let’s say a variance 1 density for simplicity). It has a simple closed form, and there is extensive literature on theoretical justifications for KDE. One conceptual difficulty with KDE is that it is not represented as a solution to an optimization problem. Most statistics and ML algorithms, from PCA to SVM to k-means are either formulated as an optimization or may be alternatively formulated as the solution to one. For me, this gives me better intuition, and often provides a decision-theoretic justification for the problem. For example, the popular ML algorithm <a href="https://en.wikipedia.org/wiki/AdaBoost#Boosting_as_gradient_descent">AdaBoost</a>, the first boosting algorithm, benefited from new insights and extensions when it was <a href="http://www.jstor.org/stable/2699986">discovered</a> that it is essentially a greedy algorithm for optimizing the exponential classification loss.</p><p>A few years ago I came across the paper <a href="http://ysidro.econ.uiuc.edu/~roger/research/densiles/heat.pdf">What do Kernel Density Estimators Optimize?</a> by Koenker et al. It has some interesting connections between the <a href="https://en.wikipedia.org/wiki/Heat_equation">heat equation</a> and KDEs, but the theorem I find most interesting is the following:</p>


        </div>
        
          <p class="post-continue">
            <a href="/2016/01/25/the-kernel-density-estimator-minimizes-the/">Read on &rarr;</a>
          </p>
        
      </li>
    
  </ul>

  
  <div class="pagination">
    
      <a class="previous" href="/posts/3/">&laquo; Older</a>
    

    
      <a class="next" href="/">Newer &raquo;</a>
    
  </div>



</div>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy;  - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="http://localhost:4000/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>
